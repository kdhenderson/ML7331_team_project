{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0ed034-09ed-4733-bbc6-730b65f9ebf2",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Build upon the classification completed in the mini-project, adding additional modeling from new classification algorithms\n",
    "- Add explanations that are inline with the CRISP-DM framework.\n",
    "- Use appropriate cross validation for all of your analysis. Explain your chosen method of performance validation in detail.\n",
    "- Try to use as much testing data as possible in a realistic manner. Define what you think\n",
    "is realistic and why.\n",
    "\n",
    "- Identify two tasks from the dataset to regress or classify. That is:  \n",
    "  - two classification tasks OR\n",
    "  - two regression tasks OR\n",
    "  - one classification task and one regression task  \n",
    "- Example from the diabetes dataset:\n",
    "  (1) Classify if a patient will be readmitted within a 30 day period or not.\n",
    "  (2) Regress what the total number of days a patient will spend in the hospital, given their history and specifics of the encounter like tests administered and previous admittance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e70a9f1-9b85-47fe-8dd0-5d34affd0c67",
   "metadata": {},
   "source": [
    "### Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b88ae6-8dd6-438c-803b-be46948f204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00470166-6b57-4f5e-b286-65b2ff70ff8a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "- Describe the final dataset that is used for classification/regression (include a\n",
    "description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d865ecb-ec75-4a06-a97e-179f2854e367",
   "metadata": {},
   "source": [
    "#### Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d95ae4-31f2-4c8a-902e-9e5aa20321ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataset\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df_clean.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df_clean[['medical_specialty', 'payer_code', 'race']] = df_clean[['medical_specialty', 'payer_code', 'race']].fillna('Unknown')\n",
    "df_clean[['diag_1', 'diag_2', 'diag_3']] = df_clean[['diag_1', 'diag_2', 'diag_3']].fillna('Unknown/None')\n",
    "df_clean[['max_glu_serum', 'A1Cresult']] = df_clean[['max_glu_serum', 'A1Cresult']].fillna('Untested')\n",
    "\n",
    "# Convert categorical integer variables to category dtype\n",
    "categorical_int_cols = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df_clean[categorical_int_cols] = df_clean[categorical_int_cols].astype('category')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_clean.drop(columns=['encounter_id', 'examide', 'citoglipton', 'weight', 'patient_nbr'], inplace=True)\n",
    "\n",
    "# Define ordinal category orders\n",
    "category_orders = {\n",
    "    'readmitted': ['<30', '>30', 'NO'],\n",
    "    'max_glu_serum': ['Untested', 'Norm', '>200', '>300'],\n",
    "    'A1Cresult': ['Untested', 'Norm', '>7', '>8'],\n",
    "    'age': ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)',\n",
    "            '[50-60)', '[60-70)', '[70-80)', '[80-90)', '[90-100)']\n",
    "}\n",
    "\n",
    "# Convert ordinal variables\n",
    "for col, order in category_orders.items():\n",
    "    df_clean[col] = pd.Categorical(df_clean[col], categories=order, ordered=True)\n",
    "\n",
    "# Convert drug variables to ordinal categories\n",
    "drug_order = ['No', 'Down', 'Steady', 'Up']\n",
    "drug_cols = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "                'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'tolazamide', \n",
    "                'pioglitazone', 'rosiglitazone', 'troglitazone', 'acarbose', 'miglitol', \n",
    "                'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "                'metformin-rosiglitazone', 'metformin-pioglitazone', 'glimepiride-pioglitazone']\n",
    "for col in drug_cols:\n",
    "    df_clean[col] = pd.Categorical(df_clean[col], categories=drug_order, ordered=True)\n",
    "\n",
    "# Preprocess diag_1, diag_2, diag_3 combining all codes with decimals under their integer values\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df_clean[col] = df_clean[col].str.split('.').str[0]  # Drop decimals and digits after\n",
    "\n",
    "df_clean.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f7ec9-d94f-4944-960f-020b0ef8abef",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09bed4-4fe7-414d-ad1b-2f3e37f1494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract response variable\n",
    "y = df_clean['readmitted']\n",
    "X = df_clean.drop(columns=['readmitted'])\n",
    "\n",
    "# Make a binary (readmitted within 30 days, 'Yes', or not, 'No') version of the response variable\n",
    "y_binary = y.copy()\n",
    "y_binary = np.where(y == '<30', 'Yes', 'No')\n",
    "\n",
    "\n",
    "# One-Hot Encoding categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)  # drop_first for multicollinearity issues - log reg\n",
    "\n",
    "# Standardize numerical features\n",
    "num_cols = X_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "X_encoded[num_cols] = scaler.fit_transform(X_encoded[num_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6c311-a94d-4a9e-bf6f-3a2194a015e9",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a1d0f-d680-4c73-a5c5-6aa6afd33710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numerical features before one-hot encoding\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "## Feature Selection from RF Variable Importance (Done in EDA)\n",
    "rf_features_js = ['num_lab_procedures', 'diag_1', 'diag_2', 'diag_3', 'num_medications', 'time_in_hospital', 'age', \n",
    "                  'number_inpatient', 'medical_specialty', 'discharge_disposition_id', 'payer_code', 'num_procedures', \n",
    "                  'number_diagnoses', 'admission_type_id', 'admission_source_id']\n",
    "rf_features_kh = ['num_lab_procedures', 'num_medications', 'time_in_hospital', 'number_inpatient', 'number_diagnoses', \n",
    "                  'num_procedures', 'number_outpatient', 'number_emergency', 'diag_3', 'gender', 'diag_1', 'medical_specialty', \n",
    "                  'diag_2', 'payer_code', 'race', 'discharge_disposition_id']\n",
    "\n",
    "# Get the union of both feature lists (combined RF-selected features)\n",
    "rf_features_all = list(set(rf_features_js) | set(rf_features_kh))\n",
    "\n",
    "print(f\"Total RF-Selected Features ({len(rf_features_all)}): {rf_features_all}\")\n",
    "\n",
    "# Alternatively, we could take only the common features\n",
    "# rf_features_common = list(set_js & set_kh)  # OR use set_js.intersection(set_kh)\n",
    "# print(f\"\\nFeatures in both RF lists ({len(rf_features_common)}):\\n\", rf_features_common)\n",
    "\n",
    "## Create Reduced Datasets with RF-Selected Features\n",
    "X_rf_selected = X[rf_features_all]  # Select the relevant features\n",
    "\n",
    "\n",
    "# # Identify Categorical Features\n",
    "# rf_features_categorical = list(set(X.select_dtypes(include=['object', 'category']).columns) & set(rf_features_all))\n",
    "# rf_features_numeric = list(set(rf_features_all) - set(rf_features_categorical))  # Keep numeric features\n",
    "\n",
    "# # Identify One-Hot Encoded Columns\n",
    "# one_hot_cols = X_encoded.columns\n",
    "\n",
    "# # Get All One-Hot Encoded Versions of Categorical Features\n",
    "# rf_encoded_features = []\n",
    "# for cat_feat in rf_features_categorical:\n",
    "#     rf_encoded_features.extend([col for col in one_hot_cols if col.startswith(cat_feat + \"_\")])\n",
    "\n",
    "# # Merge Selected Features: Numeric + One-Hot Encoded Categorical\n",
    "# rf_features_final = rf_features_numeric + rf_encoded_features\n",
    "\n",
    "# print(f\"Final RF-Selected Features Count: {len(rf_features_final)}\")\n",
    "\n",
    "# # Create the Reduced Dataset\n",
    "# X_rf_selected = X_encoded[rf_features_final]\n",
    "\n",
    "# # Check the new dataset shape\n",
    "# print(f\"RF-Selected Dataset Shape: {X_rf_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f09c13d-ae3b-40b2-b0d7-64cd53f14aa5",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "\n",
    "- Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "- Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate.\n",
    "- Create three different classification/regression models (e.g., random forest, KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric.\n",
    "- Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "- Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods.\n",
    "- Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa62ca8-5a75-4d8c-834d-5901bf58f6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split into train (80%) and holdout test (20%) - Stratified\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X_rf_selected, y_binary, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my_binary, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define categorical and numerical feature subsets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m categorical_cols \u001b[38;5;241m=\u001b[39m X_rf_selected\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split into train (80%) and holdout test (20%) - Stratified\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rf_selected, y_binary, test_size=0.2, stratify=y_binary, random_state=1234)\n",
    "\n",
    "# Define categorical and numerical feature subsets\n",
    "categorical_cols = X_rf_selected.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X_rf_selected.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Column Transformer: OneHotEncode categorical, Scale numerical\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_cols),\n",
    "    ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9d5dc-a2fc-42d1-9252-c881cacb8b73",
   "metadata": {},
   "source": [
    "### Classification Task: Predic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf378de-3cb0-4de3-8262-2c347ef3c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines\n",
    "clf_lr_pipeline = Pipeline([('preprocessor', preprocessor), ('clf', clf_lr)])\n",
    "clf_nb_pipeline = Pipeline([('preprocessor', OneHotEncoder()), ('clf', clf_nb)])  # No scaling needed\n",
    "clf_dt_pipeline = Pipeline([('preprocessor', preprocessor), ('clf', clf_dt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb574947-3f05-44b2-8bb2-b3b0b18659a8",
   "metadata": {},
   "source": [
    "#### Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabbfac-fd9e-4533-bf42-3d0a3a897948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid for MultinomialNB\n",
    "param_grid_nb = {'clf__alpha': np.arange(0.1, 1.1, 0.1)}\n",
    "\n",
    "# GridSearch for MNB\n",
    "grid_nb = GridSearchCV(clf_nb_pipeline, param_grid_nb, cv=5, scoring='recall', n_jobs=-1)\n",
    "grid_nb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha for MNB: {grid_nb.best_params_}\")\n",
    "\n",
    "# Define grid for AdaBoost\n",
    "param_grid_ab = {'clf__n_estimators': [50, 100, 200, 500]}\n",
    "\n",
    "# GridSearch for AdaBoost\n",
    "grid_ab = GridSearchCV(clf_dt_pipeline, param_grid_ab, cv=5, scoring='recall', n_jobs=-1)\n",
    "grid_ab.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best n_estimators for AdaBoost: {grid_ab.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47046000-e18d-4e87-8d8c-cd4d63327bca",
   "metadata": {},
   "source": [
    "#### SGD Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e20a86-365a-4d25-836f-b13520de3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # Initialize SGD Classifier for Logistic Regression\n",
    "#     sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", \n",
    "#                             'alpha': 1e-05, 'eta0': 0.01,\n",
    "#                             max_iter=1000, class_weight=\"balanced\",\n",
    "#                             learning_rate=\"adaptive\", n_jobs=-1, random_state=1234)\n",
    "\n",
    "#     # Ensure X_train, X_test, y_train, y_test are NumPy arrays\n",
    "#     X_train, X_test, y_train, y_test = X_train.values, X_test.values, y_train.values, y_test.values\n",
    "\n",
    "    \n",
    "#     # Perform K-Fold Cross-Validation\n",
    "#     num_folds = 5\n",
    "#     cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "#     cv_accuracies = []\n",
    "    \n",
    "#     for train_idx, val_idx in cv_object.split(X_train, y_train):\n",
    "#         X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "#         y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "#         # Train on training fold\n",
    "#         sgd_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#         # Validate on validation fold\n",
    "#         y_val_pred = sgd_clf.predict(X_val_fold)\n",
    "#         acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "#         cv_accuracies.append(acc)\n",
    "\n",
    "#     print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "#     # Train Final Model on Full Training Data\n",
    "#     sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "#     # Evaluate on Independent Test Set\n",
    "#     y_test_pred = sgd_clf.predict(X_test)\n",
    "#     test_acc = mt.accuracy_score(y_test, y_test_pred)\n",
    "#     conf_matrix = mt.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "#     print(f\"Model Converged in {sgd_clf.n_iter_} Iterations\")\n",
    "#     print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "#     print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "#     print(classification_report(y_test, y_test_pred, target_names=['<30', '>30', 'NO']))\n",
    "\n",
    "\n",
    "# Define Classifiers\n",
    "clf_lr = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", alpha=1e-05, eta0=0.01,\n",
    "                       max_iter=1000, class_weight=\"balanced\",\n",
    "                       learning_rate=\"adaptive\", n_jobs=-1, random_state=1234)\n",
    "\n",
    "clf_nb = MultinomialNB(alpha=0.5)  # Tune alpha via GridSearchCV later\n",
    "clf_dt = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy', max_depth=1),\n",
    "                            n_estimators=500)\n",
    "\n",
    "# Define classifier labels\n",
    "clf_labels = ['SGD Logistic Regression', 'Multinomial Naive Bayes', 'AdaBoost Decision Tree']\n",
    "\n",
    "# Define Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "\n",
    "# Cross-validationa\n",
    "print('10-fold cross validation (CV):\\n')\n",
    "for clf, label in zip([clf_lr_pipeline, clf_nb_pipeline, clf_dt_pipeline], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=skf, # Stratified 10-fold CV\n",
    "                             scoring='recall',  # Prioritize recall\n",
    "                             n_jobs=-1)  # Use all CPUs\n",
    "    print(f\"CV recall: {scores.mean():.2f} (+/- {scores.std():.2f}) [{label}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f37a1a-eb14-4be8-85c1-c6fe47031fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SGD Classifier for Logistic Regression\n",
    "clf_lr = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", \n",
    "                            'alpha': 1e-05, 'eta0': 0.01,\n",
    "                            max_iter=1000, class_weight=\"balanced\",\n",
    "                            learning_rate=\"adaptive\", n_jobs=-1, random_state=1234)\n",
    "clf_mnb = MultinomialNB(alpha=alpha), # alphas = np.arange(0.1, 1.1, 0.1)\n",
    "clf_abdt = ... (tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              max_depth=1),\n",
    "              ada = AdaBoostClassifier(base_estimator=tree,\n",
    "                         n_estimators=500) # maybe also tune hyperparameters\n",
    "\n",
    "clf_lr_pipeline = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf', clf_lr]])\n",
    "\n",
    "# define classifier labels\n",
    "clf_labels = ['SGD Logistic Regression', 'Multinomial Naive Bayes', 'AdaBoost Decision Tree']\n",
    "\n",
    "# evaluate the model performance for each classifier using 10-fold cross validation on the training data\n",
    "# note that with the 10-fold validation we don't try to find the optimal combination of hyperparameter values (i.e., use the GridSearchCV() method from sklearn.model_selection module)\n",
    "# instead, we want to fine-tune the performance given a single set of hyperparameter values\n",
    "print('10-fold cross validation (CV):\\n')\n",
    "for clf, label in zip([clf_lr_pipeline, clf_nb, clf_dt], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             n_jobs=1) # n_jobs = the number of CPUs to use, set to -1 to use all\n",
    "    print(\"CV accuracy: %0.2f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label)) # cross_val_score() returns stats(e.g., mean and variance) for accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d941a0b-ce37-40a9-a8b3-2e9dc041851f",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "- How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778554c7-51ed-441a-bebe-746351fe2b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ddeb0c-8546-471f-bcfd-1c89e086f161",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "- You have free reign to provide additional modeling.\n",
    "- One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18d880-2f70-49e0-988c-93ff55d53a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
