{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3769c0af-ad39-4a74-b3ef-a06566888c2c",
   "metadata": {},
   "source": [
    "## MSDS 7331 Mini Lab Two: Logistic Regression and Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4cb82e",
   "metadata": {},
   "source": [
    "### Authors: Jaren Shead, Kristin Henderson, Tom Hines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d7ccf-1532-481e-9a58-44910596da0c",
   "metadata": {},
   "source": [
    "### Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882210b0-bb1c-41ed-ac3a-73a5ab737cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3e697f-d6f7-4b54-be4b-b404de47004e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>patient_nbr</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>...</th>\n",
       "      <th>citoglipton</th>\n",
       "      <th>insulin</th>\n",
       "      <th>glyburide-metformin</th>\n",
       "      <th>glipizide-metformin</th>\n",
       "      <th>glimepiride-pioglitazone</th>\n",
       "      <th>metformin-rosiglitazone</th>\n",
       "      <th>metformin-pioglitazone</th>\n",
       "      <th>change</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>?</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>&gt;30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Up</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Steady</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
       "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
       "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
       "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
       "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
       "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
       "\n",
       "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
       "0                  6                        25                    1   \n",
       "1                  1                         1                    7   \n",
       "2                  1                         1                    7   \n",
       "3                  1                         1                    7   \n",
       "4                  1                         1                    7   \n",
       "\n",
       "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
       "0                 1  ...          No      No                   No   \n",
       "1                 3  ...          No      Up                   No   \n",
       "2                 2  ...          No      No                   No   \n",
       "3                 2  ...          No      Up                   No   \n",
       "4                 1  ...          No  Steady                   No   \n",
       "\n",
       "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
       "0                   No                        No                       No   \n",
       "1                   No                        No                       No   \n",
       "2                   No                        No                       No   \n",
       "3                   No                        No                       No   \n",
       "4                   No                        No                       No   \n",
       "\n",
       "   metformin-pioglitazone  change diabetesMed readmitted  \n",
       "0                      No      No          No         NO  \n",
       "1                      No      Ch         Yes        >30  \n",
       "2                      No      No         Yes         NO  \n",
       "3                      No      Ch         Yes         NO  \n",
       "4                      No      Ch         Yes         NO  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d39b1-2c43-4884-b8c4-1c03925a7328",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c93c75-888d-45a8-92ff-a54fc9e002e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `?` with NaN for now\n",
    "df_clean = df.copy()               # make a copy of df called df_clean and then clean it\n",
    "df_clean = df.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea220b25-0ec3-4263-988a-3ff9bf701374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN ('?') with 'Unknown' in the specified columns\n",
    "columns_to_update_1 = ['medical_specialty', 'payer_code', 'race']\n",
    "df_clean[columns_to_update_1] = df_clean[columns_to_update_1].replace(np.nan, 'Unknown')\n",
    "\n",
    "# Replace NaN ('?') with 'Unknown' in the specified columns\n",
    "columns_to_update_2 = ['diag_1', 'diag_2', 'diag_3']\n",
    "df_clean[columns_to_update_2] = df_clean[columns_to_update_2].replace(np.nan, 'Unknown/None')\n",
    "\n",
    "# Replace NaN with 'Untested' in the specified columns\n",
    "columns_to_update_3 = ['max_glu_serum', 'A1Cresult']\n",
    "df_clean[columns_to_update_3] = df_clean[columns_to_update_3].replace(np.nan, 'Untested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a21274-5d9d-4bdb-b842-fc5a6aa0fe3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_nbr                 category\n",
       "admission_type_id           category\n",
       "discharge_disposition_id    category\n",
       "admission_source_id         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert categorical variables `patient_nbr`, `admission_type_id`, `discharge_disposition_id`, `admission_source_id`\n",
    "# from integer to object datatype.\n",
    "categoricalInt_cols = ['patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df_clean[categoricalInt_cols] = df_clean[categoricalInt_cols].astype('category')\n",
    "df_clean[['patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e32ce81-cc5b-476f-ba7b-114ec32e5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Data columns (total 46 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   patient_nbr               101766 non-null  category\n",
      " 1   race                      101766 non-null  object  \n",
      " 2   gender                    101766 non-null  object  \n",
      " 3   age                       101766 non-null  object  \n",
      " 4   admission_type_id         101766 non-null  category\n",
      " 5   discharge_disposition_id  101766 non-null  category\n",
      " 6   admission_source_id       101766 non-null  category\n",
      " 7   time_in_hospital          101766 non-null  int64   \n",
      " 8   payer_code                101766 non-null  object  \n",
      " 9   medical_specialty         101766 non-null  object  \n",
      " 10  num_lab_procedures        101766 non-null  int64   \n",
      " 11  num_procedures            101766 non-null  int64   \n",
      " 12  num_medications           101766 non-null  int64   \n",
      " 13  number_outpatient         101766 non-null  int64   \n",
      " 14  number_emergency          101766 non-null  int64   \n",
      " 15  number_inpatient          101766 non-null  int64   \n",
      " 16  diag_1                    101766 non-null  object  \n",
      " 17  diag_2                    101766 non-null  object  \n",
      " 18  diag_3                    101766 non-null  object  \n",
      " 19  number_diagnoses          101766 non-null  int64   \n",
      " 20  max_glu_serum             101766 non-null  object  \n",
      " 21  A1Cresult                 101766 non-null  object  \n",
      " 22  metformin                 101766 non-null  object  \n",
      " 23  repaglinide               101766 non-null  object  \n",
      " 24  nateglinide               101766 non-null  object  \n",
      " 25  chlorpropamide            101766 non-null  object  \n",
      " 26  glimepiride               101766 non-null  object  \n",
      " 27  acetohexamide             101766 non-null  object  \n",
      " 28  glipizide                 101766 non-null  object  \n",
      " 29  glyburide                 101766 non-null  object  \n",
      " 30  tolbutamide               101766 non-null  object  \n",
      " 31  pioglitazone              101766 non-null  object  \n",
      " 32  rosiglitazone             101766 non-null  object  \n",
      " 33  acarbose                  101766 non-null  object  \n",
      " 34  miglitol                  101766 non-null  object  \n",
      " 35  troglitazone              101766 non-null  object  \n",
      " 36  tolazamide                101766 non-null  object  \n",
      " 37  insulin                   101766 non-null  object  \n",
      " 38  glyburide-metformin       101766 non-null  object  \n",
      " 39  glipizide-metformin       101766 non-null  object  \n",
      " 40  glimepiride-pioglitazone  101766 non-null  object  \n",
      " 41  metformin-rosiglitazone   101766 non-null  object  \n",
      " 42  metformin-pioglitazone    101766 non-null  object  \n",
      " 43  change                    101766 non-null  object  \n",
      " 44  diabetesMed               101766 non-null  object  \n",
      " 45  readmitted                101766 non-null  object  \n",
      "dtypes: category(4), int64(8), object(34)\n",
      "memory usage: 35.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Remove encounter id, examide, citoglipton and weight from the dataset.\n",
    "df_clean = df_clean.drop(columns=['encounter_id'])            # id variable\n",
    "df_clean = df_clean.drop(columns=['examide', 'citoglipton'])  # zero variance\n",
    "df_clean = df_clean.drop(columns=['weight'])                  # high percent missing\n",
    "\n",
    "print( df_clean.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e3ef68-6978-4852-aa85-8c060fa1d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readmitted\n",
      "NO     0.539119\n",
      ">30    0.349282\n",
      "<30    0.111599\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Find the proportion of each response class.\n",
    "print(df_clean['readmitted'].value_counts()/df_clean['readmitted'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f92f0-1709-48d5-9a29-5bf0c244f51a",
   "metadata": {},
   "source": [
    "The response class is unbalanced with 54% patient records not being readmitted, 35% being readmitted in greater than 30 days, and 11% being readmitted in less than 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff582e8-bea8-48e4-b401-d578c5d04172",
   "metadata": {},
   "source": [
    "#### Data preprocessing: Explicitly set the order of ordinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f77049-f585-4158-b2dc-09b17003e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct order for each variable\n",
    "readmit_order = ['<30', '>30', 'NO']\n",
    "drug_order = ['No', 'Down', 'Steady', 'Up']\n",
    "max_glu_serum_order = ['Untested', 'Norm', '>200', '>300']\n",
    "a1cresult_order = ['Untested', 'Norm', '>7', '>8']\n",
    "age_order = ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)',\n",
    "             '[50-60)', '[60-70)', '[70-80)', '[80-90)', '[90-100)'] \n",
    "\n",
    "# List of drug-related variables\n",
    "drug_columns = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "                'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'tolazamide', \n",
    "                'pioglitazone', 'rosiglitazone', 'troglitazone', 'acarbose', 'miglitol', \n",
    "                'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "                'metformin-rosiglitazone', 'metformin-pioglitazone', 'glimepiride-pioglitazone']\n",
    "\n",
    "# Reorder categories in the DataFrame\n",
    "df_clean['readmitted'] = pd.Categorical(df_clean['readmitted'], categories=readmit_order, ordered=True)\n",
    "df_clean['max_glu_serum'] = pd.Categorical(df_clean['max_glu_serum'], categories=max_glu_serum_order, ordered=True)\n",
    "df_clean['A1Cresult'] = pd.Categorical(df_clean['A1Cresult'], categories=a1cresult_order, ordered=True)\n",
    "df_clean['age'] = pd.Categorical(df_clean['age'], categories=age_order, ordered=True)\n",
    "\n",
    "for col in drug_columns:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.Categorical(df_clean[col], categories=drug_order, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69883a79-a885-4dd9-8a77-fc5de3a3aa5f",
   "metadata": {},
   "source": [
    "#### One Hot Encoding the Full Dataset for SVG Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f321622e-0858-48dd-b0ec-6e44e268ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding (drop first for logistic regression\n",
    "\n",
    "# Remove columns with many missing values and patient IDs\n",
    "columns_to_drop = ['patient_nbr'] #, 'max_glu_serum', 'A1Cresult']\n",
    "df_encoded = df_clean.drop(columns= columns_to_drop)             # make a copy of df_clean called df_encoded dropping patient_nbr (id variable)\n",
    "\n",
    "# Preprocess diag_1, diag_2, diag_3 combining all codes with decimals under their integer values \n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df_encoded[col] = df_encoded[col].str.split('.').str[0]  # Drop decimals and digits after\n",
    "# print(df_encoded[['diag_1', 'diag_2', 'diag_3']].head(20))\n",
    "\n",
    "# # check for missing values\n",
    "# print(df_encoded.isna().sum())\n",
    "# # Drop rows with NaN values\n",
    "# df_encoded = df_encoded.dropna()\n",
    "\n",
    "# Remove target (split into X and y, for plotting with y later)\n",
    "X_df_encoded = df_encoded.drop(columns='readmitted')\n",
    "y_df_encoded = df_encoded['readmitted']\n",
    "\n",
    "# Handle categorical variables with one-hot encoding\n",
    "categorical_columns = X_df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "X_df_encoded = pd.get_dummies(X_df_encoded, columns=categorical_columns, drop_first=True) # drop_first for multicollinearity issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bebb7d5-f065-43c6-93e8-0dceeb594ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Columns: 2387 entries, time_in_hospital to diabetesMed_Yes\n",
      "dtypes: bool(2379), int64(8)\n",
      "memory usage: 237.1 MB\n"
     ]
    }
   ],
   "source": [
    "X_df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1063c859-6a86-4830-8e3a-66027c052870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features from RFE:\n",
      "['discharge_disposition_id_11' 'discharge_disposition_id_13'\n",
      " 'discharge_disposition_id_14'\n",
      " 'medical_specialty_Endocrinology-Metabolism' 'diag_1_227' 'diag_1_242'\n",
      " 'diag_1_378' 'diag_1_967' 'diag_2_421' 'diag_2_513']\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection with RFECV\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Identify numerical features (categoricals are already one-hot encoded)\n",
    "numeric_features = X_df_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "all_features = X_df_encoded.columns  # One-hot encoded features are already included\n",
    "\n",
    "# Scale numerical features but keep categorical features unchanged\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features)  # Scale only numerical features\n",
    "], remainder='passthrough')  # Keep categorical (one-hot encoded) features unchanged\n",
    "\n",
    "# Apply transformation\n",
    "X_scaled = preprocessor.fit_transform(X_df_encoded)\n",
    "\n",
    "# Ensure `feature_names` correctly represents all transformed features\n",
    "feature_names = list(all_features)\n",
    "\n",
    "# Ensure feature names match the dataset shape\n",
    "assert len(feature_names) == X_scaled.shape[1], f\"Feature names length ({len(feature_names)}) does not match dataset shape ({X_scaled.shape[1]})!\"\n",
    "\n",
    "# Split into train (80%) and test (20%) - Stratified\n",
    "Xrfecv_train, Xrfecv_test, yrfecv_train, yrfecv_test = train_test_split(X_scaled, y_df_encoded, test_size=0.2, stratify=y_df_encoded, random_state=1234)\n",
    "\n",
    "# Define a fast logistic regression model with L1 regularization\n",
    "sgd_model = SGDClassifier(loss='log_loss', penalty='l1', max_iter=1000, random_state=1234)\n",
    "\n",
    "# Apply RFECV to automatically select the optimal number of features\n",
    "rfecv = RFECV(estimator=sgd_model, step=0.2, cv=5, scoring='accuracy', n_jobs=-1)  # Adjust scoring if needed\n",
    "X_rfecv = rfecv.fit_transform(Xrfecv_train, yrfecv_train)\n",
    "\n",
    "# Get selected feature mask\n",
    "selected_mask = rfecv.support_\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_rfecv = np.array(feature_names)[selected_mask]\n",
    "\n",
    "print(f\"\\nOptimal number of features selected: {rfecv.n_features_}\")\n",
    "print(\"\\nSelected Features from RFECV:\")\n",
    "print(selected_features_rfecv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dccbb1-15fd-4864-92a2-518324f4baa3",
   "metadata": {},
   "source": [
    "#### Reduce Dataset to Important Variables for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2cbd000-555d-4e98-a208-76fc28918f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique features (19):\n",
      " ['diag_1', 'num_procedures', 'age', 'time_in_hospital', 'admission_source_id', 'num_medications', 'gender', 'number_inpatient', 'payer_code', 'admission_type_id', 'diag_2', 'diag_3', 'medical_specialty', 'number_outpatient', 'discharge_disposition_id', 'number_emergency', 'race', 'number_diagnoses', 'num_lab_procedures']\n",
      "\n",
      "Features in both lists (12):\n",
      " ['diag_2', 'medical_specialty', 'diag_3', 'num_procedures', 'diag_1', 'discharge_disposition_id', 'time_in_hospital', 'number_diagnoses', 'num_medications', 'number_inpatient', 'num_lab_procedures', 'payer_code']\n"
     ]
    }
   ],
   "source": [
    "rf_features_js = ['num_lab_procedures', 'diag_1', 'diag_2', 'diag_3', 'num_medications', 'time_in_hospital', 'age', \n",
    "                  'number_inpatient', 'medical_specialty', 'discharge_disposition_id', 'payer_code', 'num_procedures', \n",
    "                  'number_diagnoses', 'admission_type_id', 'admission_source_id']\n",
    "rf_features_kh = ['num_lab_procedures', 'num_medications', 'time_in_hospital', 'number_inpatient', 'number_diagnoses', \n",
    "                  'num_procedures', 'number_outpatient', 'number_emergency', 'diag_3', 'gender', 'diag_1', 'medical_specialty', \n",
    "                  'diag_2', 'payer_code', 'race', 'discharge_disposition_id']\n",
    "\n",
    "# Convert lists to sets\n",
    "set_js = set(rf_features_js)\n",
    "set_kh = set(rf_features_kh)\n",
    "\n",
    "# List of all unique features (Union)\n",
    "rf_features_all = list(set_js | set_kh)  # OR use set_js.union(set_kh)\n",
    "\n",
    "# List of only common features (Intersection)\n",
    "rf_features_common = list(set_js & set_kh)  # OR use set_js.intersection(set_kh)\n",
    "\n",
    "# Print results\n",
    "print(f\"All unique features ({len(rf_features_all)}):\\n\", rf_features_all)\n",
    "print(f\"\\nFeatures in both lists ({len(rf_features_common)}):\\n\", rf_features_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3268baa0-ef9e-4574-a892-5b0e85ef137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Data columns (total 19 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   diag_1                    101766 non-null  object  \n",
      " 1   num_procedures            101766 non-null  int64   \n",
      " 2   age                       101766 non-null  category\n",
      " 3   time_in_hospital          101766 non-null  int64   \n",
      " 4   admission_source_id       101766 non-null  category\n",
      " 5   num_medications           101766 non-null  int64   \n",
      " 6   gender                    101766 non-null  object  \n",
      " 7   number_inpatient          101766 non-null  int64   \n",
      " 8   payer_code                101766 non-null  object  \n",
      " 9   admission_type_id         101766 non-null  category\n",
      " 10  diag_2                    101766 non-null  object  \n",
      " 11  diag_3                    101766 non-null  object  \n",
      " 12  medical_specialty         101766 non-null  object  \n",
      " 13  number_outpatient         101766 non-null  int64   \n",
      " 14  discharge_disposition_id  101766 non-null  category\n",
      " 15  number_emergency          101766 non-null  int64   \n",
      " 16  race                      101766 non-null  object  \n",
      " 17  number_diagnoses          101766 non-null  int64   \n",
      " 18  num_lab_procedures        101766 non-null  int64   \n",
      "dtypes: category(4), int64(8), object(7)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the cleaned dataset to reduce further\n",
    "df_reduced = df_clean.copy()\n",
    "\n",
    "# Preprocess diag_1, diag_2, diag_3 combining all codes with decimals under their integer values \n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df_reduced[col] = df_reduced[col].str.split('.').str[0]  # Drop decimals and digits after\n",
    "\n",
    "# split into X and y\n",
    "X_reduced = df_reduced.drop(columns='readmitted')\n",
    "y_reduced = df_reduced['readmitted']\n",
    "\n",
    "# List of important features to keep in reduced dataset\n",
    "top_features = rf_features_all\n",
    "# top_features = rf_features_common # option for a smaller feature list if needed\n",
    "\n",
    "# Keep only selected features BEFORE one-hot encoding\n",
    "X_reduced = X_reduced[top_features]\n",
    "X_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e548a50-c55a-470d-b419-8a43af21517b",
   "metadata": {},
   "source": [
    "#### Encode Reduced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63dd88dd-0225-4458-8bc6-54670a933093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape after one-hot encoding: (101766, 2316)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Columns: 2316 entries, num_procedures to race_Unknown\n",
      "dtypes: bool(2308), int64(8)\n",
      "memory usage: 230.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Select categorical columns in the reduced dataset\n",
    "categorical_columns = X_reduced.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Apply One-Hot Encoding\n",
    "X_reduced_encoded = pd.get_dummies(X_reduced, columns=categorical_columns, drop_first=True)  # drop_first=True avoids multicollinearity\n",
    "\n",
    "print(f\"Final shape after one-hot encoding: {X_reduced_encoded.shape}\")\n",
    "\n",
    "X_reduced_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40309af9-0da1-4139-bc84-8a204b0c2526",
   "metadata": {},
   "source": [
    "#### Standardize (only numerical variables) for `saga`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6c6a63-c62d-4032-ac41-46aab6d1bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numerical columns only\n",
    "num_features = X_reduced_encoded.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# # Scale all features\n",
    "# X_reduced_scaled = sc.fit_transform(X_reduced_encoded)  # Scale all features (numeric + encoded)\n",
    "\n",
    "# Scale only numerical features\n",
    "X_reduced_scaled_num = sc.fit_transform(X_reduced_encoded[num_features])  # Scale only numerical features\n",
    "# Convert back to DataFrame\n",
    "X_reduced_scaled_num = pd.DataFrame(X_reduced_scaled_num, columns=num_features, index=X_reduced_encoded.index)\n",
    "# Keep categorical (one-hot encoded) variables unchanged\n",
    "X_reduced_scaled = pd.concat([X_reduced_scaled_num, X_reduced_encoded.drop(columns=num_features)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5c5ae-f064-4684-810a-f496f07ba0f9",
   "metadata": {},
   "source": [
    "#### Train/Test Splits for Full Dataset and for Reduced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080693f6-7626-4a22-88cb-2fc02cfe4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "import numpy as np\n",
    "\n",
    "# Convert DataFrames to numpy arrays, full dataset\n",
    "y = y_df_encoded.values\n",
    "X = X_df_encoded.values\n",
    "# Split into train (80%) and test (20%) - Stratified\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1234)\n",
    "\n",
    "\n",
    "# Dataset with reduced features\n",
    "y_reduced = y_reduced.values # convert to np.array if only num features scaled?\n",
    "# X_reduced = X_reduced_scaled # np.array if all features scaled\n",
    "X_reduced = X_reduced_scaled.values # dataframe if only num features scaled\n",
    "Xred_train, Xred_test, yred_train, yred_test = train_test_split(X_reduced, y_reduced, test_size=0.2, stratify=y_reduced, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b038d4-d40c-4f8b-9ad0-cb296cc7a7d8",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Try different solvers: saga which may need more iters and scaled data, and lbfgs.  \n",
    "Also, try reducing the features to use only the important variables from random forest from EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00f3c1ac-321d-4782-b5d5-1a46506aac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8ca1381-d2a3-45df-872a-707b1ce29592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Accuracy: 0.558\n",
      "Results from model with 'optimal' learning_rate parameter.\n",
      "Model converged in 23 iterations.\n",
      "Final Model Test Accuracy: 0.563\n",
      "Confusion Matrix:\n",
      "[[ 356 1042  874]\n",
      " [ 490 3595 3024]\n",
      " [ 509 2952 7512]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         <30       0.26      0.16      0.20      2272\n",
      "         >30       0.47      0.51      0.49      7109\n",
      "          NO       0.66      0.68      0.67     10973\n",
      "\n",
      "    accuracy                           0.56     20354\n",
      "   macro avg       0.46      0.45      0.45     20354\n",
      "weighted avg       0.55      0.56      0.55     20354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a Logistic Regression SGD Classifier with reduced dataset\n",
    "\n",
    "# Initialize SGD Classifier for Logistic Regression\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", \n",
    "                        max_iter=1000, class_weight=\"balanced\",\n",
    "                        learning_rate=\"optimal\",\n",
    "                        n_jobs=-1, random_state=1234)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "num_folds = 5\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = Xred_train[train_idx], Xred_train[val_idx]\n",
    "    y_train_fold, y_val_fold = yred_train[train_idx], yred_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    sgd_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = sgd_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "sgd_clf.fit(Xred_train, yred_train)\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = sgd_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(\"Results from model with 'optimal' learning_rate parameter.\")\n",
    "print(f\"Model converged in {sgd_clf.n_iter_} iterations.\")\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(classification_report(yred_test, y_test_pred, target_names=['<30', '>30', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19eef35f-b712-4af7-8993-ba2e2a373db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] END alpha=1e-05, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=1e-05, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=1e-05, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=1e-05, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.6s\n",
      "[CV] END alpha=1e-05, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.2s\n",
      "[CV] END alpha=1e-05, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.2s\n",
      "[CV] END alpha=1e-05, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  13.5s\n",
      "[CV] END alpha=1e-05, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  12.4s\n",
      "[CV] END alpha=1e-05, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  12.9s\n",
      "[CV] END alpha=1e-05, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.6s\n",
      "[CV] END alpha=1e-05, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.6s\n",
      "[CV] END alpha=1e-05, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.8s\n",
      "[CV] END alpha=1e-05, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.9s\n",
      "[CV] END alpha=1e-05, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.6s\n",
      "[CV] END alpha=1e-05, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.7s\n",
      "[CV] END alpha=0.0001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=0.0001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=0.0001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.6s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.3s\n",
      "[CV] END alpha=0.0001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.3s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  12.4s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  12.5s\n",
      "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  12.5s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.2s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.7s\n",
      "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.8s\n",
      "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.9s\n",
      "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  19.3s\n",
      "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.6s\n",
      "[CV] END alpha=0.001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=0.001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=0.001, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.8s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.3s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.2s\n",
      "[CV] END alpha=0.001, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  11.3s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.9s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.7s\n",
      "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.7s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.1s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.6s\n",
      "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.6s\n",
      "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  18.9s\n",
      "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  18.9s\n",
      "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  18.9s\n",
      "[CV] END alpha=0.01, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.6s\n",
      "[CV] END alpha=0.01, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.6s\n",
      "[CV] END alpha=0.01, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.5s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.2s\n",
      "[CV] END alpha=0.01, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.2s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.5s\n",
      "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.9s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.4s\n",
      "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.4s\n",
      "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  18.9s\n",
      "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  18.8s\n",
      "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  17.2s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=0.1, eta0=0.0001, learning_rate=adaptive, max_iter=1000; total time=   8.7s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.1s\n",
      "[CV] END alpha=0.1, eta0=0.001, learning_rate=adaptive, max_iter=1000; total time=  10.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.3s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.1s\n",
      "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, max_iter=1000; total time=  11.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.0s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  15.2s\n",
      "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, max_iter=1000; total time=  14.4s\n",
      "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  16.2s\n",
      "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  15.4s\n",
      "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, max_iter=1000; total time=  16.3s\n",
      "Best parameters: {'alpha': 0.0001, 'eta0': 1.0, 'learning_rate': 'adaptive', 'max_iter': 1000}\n",
      "Best CV Accuracy: 0.583\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    'eta0': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'max_iter': [1000],  # Adjust if needed\n",
    "}\n",
    "\n",
    "# Initialize SGDClassifier\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", n_jobs=-1, random_state=1234)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(sgd_clf, param_grid, cv=3, scoring=\"accuracy\", verbose=2)\n",
    "grid_search.fit(Xred_train, yred_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d89bb33-5a98-4c3d-844d-d056ce6f1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<30' '>30' 'NO']\n"
     ]
    }
   ],
   "source": [
    "# Checking the order of response classes\n",
    "print(np.unique(yred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58cc719a-07d9-4e08-ab5e-efbb85234f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Accuracy: 0.562\n",
      "Model converged in 73 iterations.\n",
      "Final Model Test Accuracy: 0.569\n",
      "Confusion Matrix:\n",
      "[[ 477  823  972]\n",
      " [ 674 2989 3446]\n",
      " [ 640 2209 8124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         <30       0.27      0.21      0.23      2272\n",
      "         >30       0.50      0.42      0.46      7109\n",
      "          NO       0.65      0.74      0.69     10973\n",
      "\n",
      "    accuracy                           0.57     20354\n",
      "   macro avg       0.47      0.46      0.46     20354\n",
      "weighted avg       0.55      0.57      0.56     20354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a Logistic Regression SGD Classifier with reduced dataset\n",
    "\n",
    "# Initialize SGD Classifier for Logistic Regression\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", \n",
    "                        max_iter=1000, class_weight=\"balanced\", # balanced class weight decreases overall accuracy but improves performance for <30 and >30 classes\n",
    "                        learning_rate=\"adaptive\", eta0=1.0,     # adaptive seems to perform better than optimal\n",
    "                        n_jobs=-1, random_state=1234)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "num_folds = 5\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = Xred_train[train_idx], Xred_train[val_idx]\n",
    "    y_train_fold, y_val_fold = yred_train[train_idx], yred_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    sgd_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = sgd_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "sgd_clf.fit(Xred_train, yred_train)\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = sgd_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Model converged in {sgd_clf.n_iter_} iterations.\")\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(classification_report(yred_test, y_test_pred, target_names=['<30', '>30', 'NO']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73cc4222-affc-49e2-a167-0aa63aa38b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Accuracy: 0.571\n",
      "Final Model Test Accuracy: 0.579\n",
      "Confusion Matrix:\n",
      "[[  48  876 1348]\n",
      " [  38 2798 4273]\n",
      " [  19 2010 8944]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define mini-batch size\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "num_folds = 3\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = Xred_train[train_idx], Xred_train[val_idx]\n",
    "    y_train_fold, y_val_fold = yred_train[train_idx], yred_train[val_idx]\n",
    "\n",
    "    # Reinitialize model in each fold (important!)\n",
    "    sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=1, learning_rate=\"optimal\", n_jobs=-1, random_state=1234, warm_start=True)\n",
    "\n",
    "    # Mini-batch SGD Training (Per Fold)\n",
    "    for epoch in range(num_epochs):\n",
    "        X_train_fold, y_train_fold = shuffle(X_train_fold, y_train_fold, random_state=epoch)\n",
    "        for i in range(0, X_train_fold.shape[0], batch_size):\n",
    "            X_batch = X_train_fold[i:i+batch_size]\n",
    "            y_batch = y_train_fold[i:i+batch_size]\n",
    "            sgd_clf.partial_fit(X_batch, y_batch, classes=np.unique(yred_train))\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = sgd_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=1, learning_rate=\"optimal\", n_jobs=-1, random_state=1234, warm_start=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    Xred_train, yred_train = shuffle(Xred_train, yred_train, random_state=epoch)\n",
    "    for i in range(0, Xred_train.shape[0], batch_size):\n",
    "        X_batch = Xred_train[i:i+batch_size]\n",
    "        y_batch = yred_train[i:i+batch_size]\n",
    "        sgd_clf.partial_fit(X_batch, y_batch, classes=np.unique(yred_train))\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = sgd_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(classification_report(yred_test, y_test_pred, target_names=['<30', '>30', 'NO']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "154069b3-cb42-4035-9318-a413564b18e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         <30       0.46      0.02      0.04      2272\n",
      "         >30       0.49      0.39      0.44      7109\n",
      "          NO       0.61      0.82      0.70     10973\n",
      "\n",
      "    accuracy                           0.58     20354\n",
      "   macro avg       0.52      0.41      0.39     20354\n",
      "weighted avg       0.55      0.58      0.53     20354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yred_test, y_test_pred, target_names=['<30', '>30', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae1acb8-9451-4e11-807b-dd2040b129e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Mean Accuracy: 0.535\n",
      "Final Model Test Accuracy: 0.589\n",
      "Confusion Matrix:\n",
      "[[  25  920 1327]\n",
      " [  11 2759 4339]\n",
      " [   2 1776 9195]]\n"
     ]
    }
   ],
   "source": [
    "# Use a Logistic Regression SGD Classifier with full dataset\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Initialize SGD Classifier for Logistic Regression\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=500, learning_rate=\"optimal\", n_jobs=-1, random_state=1234)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "num_folds = 3\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "for train_idx, val_idx in cv_object.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    sgd_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = sgd_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = sgd_clf.predict(X_test)\n",
    "test_acc = mt.accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['<30', '>30', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d1dc8cb-d883-497b-ac6e-8da5b6e7c2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed.\n",
      "Epoch 2/10 completed.\n",
      "Epoch 3/10 completed.\n",
      "Epoch 4/10 completed.\n",
      "Epoch 5/10 completed.\n",
      "Epoch 6/10 completed.\n",
      "Epoch 7/10 completed.\n",
      "Epoch 8/10 completed.\n",
      "Epoch 9/10 completed.\n",
      "Epoch 10/10 completed.\n",
      "Final Model Test Accuracy: 0.583\n",
      "Confusion Matrix:\n",
      "[[  68  733 1471]\n",
      " [  47 2267 4795]\n",
      " [  27 1420 9526]]\n"
     ]
    }
   ],
   "source": [
    "# Mini-Batch SGD Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Define mini-batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Initialize SGD Classifier for Logistic Regression\n",
    "sgd_clf = SGDClassifier(loss=\"log_loss\", penalty=\"l2\", max_iter=1, learning_rate=\"optimal\", n_jobs=-1, random_state=1234, warm_start=True)\n",
    "\n",
    "# Number of epochs (how many times to go through the data)\n",
    "num_epochs = 10\n",
    "\n",
    "# Train using mini-batch updates\n",
    "for epoch in range(num_epochs):\n",
    "    Xred_train, yred_train = shuffle(Xred_train, yred_train, random_state=epoch)  # Shuffle at the start of each epoch\n",
    "    for i in range(0, Xred_train.shape[0], batch_size):\n",
    "        X_batch = Xred_train[i:i+batch_size]\n",
    "        y_batch = yred_train[i:i+batch_size]\n",
    "        \n",
    "        # Perform one mini-batch update\n",
    "        sgd_clf.partial_fit(X_batch, y_batch, classes=np.unique(yred_train))\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "# Evaluate on the Test Set\n",
    "y_test_pred = sgd_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5641991-db15-47c7-9fb8-0303498dd4d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m y_train[train_idx], y_train[val_idx]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train on training fold\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mlr_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Validate on validation fold\u001b[39;00m\n\u001b[1;32m     21\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m lr_clf\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform K-Fold Cross-Validation on Train Set\n",
    "num_folds = 3\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234) # stratify the response classes\n",
    "\n",
    "# Initialize logistic regression model\n",
    "# solver: try saga for very large and sparse datasets and lbfgs (default) for small/medium datasets (liblinear for binary response)\n",
    "# penalty='l2' and C=1.0 are default, tune max_iter depending on convergence\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, solver='saga', max_iter=5000, n_jobs=-1)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "# Split the 80% train set into train/validation splits for K-fold CV\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    lr_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = lr_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "lr_clf.fit(Xred_train, yred_train)\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = lr_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960054b1-3a18-40a2-a39f-c608568c3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Fold Cross-Validation on Train Set\n",
    "num_folds = 3\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234) # stratify the response classes\n",
    "\n",
    "# Initialize logistic regression model\n",
    "# solver: try saga for very large and sparse datasets and lbfgs (default) for small/medium datasets (liblinear for binary response)\n",
    "# penalty='l2' and C=1.0 are default, tune max_iter depending on convergence\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=5000, n_jobs=-1)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "# Split the 80% train set into train/validation splits for K-fold CV\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    lr_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = lr_clf.predict(X_val_fold)\n",
    "    acc = mt.accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "lr_clf.fit(Xred_train, yred_train)\n",
    "\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = lr_clf.predict(Xred_test)\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6685b1f-86a5-4dc7-8a01-a534dcd88c94",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89146282-8fe6-4c54-a6c9-5540f4ee0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Let's try a linear model\n",
    "\n",
    "# Train\n",
    "# Perform K-Fold Cross-Validation on Train Set\n",
    "num_folds = 3\n",
    "cv_object = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1234) # stratify the response classes\n",
    "\n",
    "# Initialize SVM model\n",
    "svm = SVC(kernel='linear', random_state=1, C=1.0)\n",
    "\n",
    "cv_accuracies = []\n",
    "\n",
    "# Split the 80% train set into train/validation splits for K-fold CV\n",
    "for train_idx, val_idx in cv_object.split(Xred_train, yred_train):\n",
    "    X_train_fold, X_val_fold = Xred_train[train_idx], Xred_train[val_idx]\n",
    "    y_train_fold, y_val_fold = yred_train[train_idx], yred_train[val_idx]\n",
    "\n",
    "    # Train on training fold\n",
    "    svm.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Validate on validation fold\n",
    "    y_val_pred = svm.predict(X_val_fold)\n",
    "    acc = accuracy_score(y_val_fold, y_val_pred)\n",
    "    cv_accuracies.append(acc)\n",
    "\n",
    "print(f\"Cross-Validation Mean Accuracy: {np.mean(cv_accuracies):.3f}\")\n",
    "\n",
    "# Train Final Model on Full Training Data\n",
    "svm.fit(Xred_train, yred_train)\n",
    "\n",
    "# Predict\n",
    "# Evaluate on Independent Test Set\n",
    "y_test_pred = svm.predict(Xred_test)\n",
    "print('Length y_pred: ', len(y_test_pred))\n",
    "test_acc = mt.accuracy_score(yred_test, y_test_pred)\n",
    "conf_matrix = mt.confusion_matrix(yred_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Model Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(classification_report(yred_test, y_test_pred, target_names=['<30', '>30', 'NO']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422eb8c-dbb6-45a8-9365-a48a7215f874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
