{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3769c0af-ad39-4a74-b3ef-a06566888c2c",
   "metadata": {},
   "source": [
    "## Lab One: Visualization and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d7ccf-1532-481e-9a58-44910596da0c",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882210b0-bb1c-41ed-ac3a-73a5ab737cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3e697f-d6f7-4b54-be4b-b404de47004e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/rhine/Desktop/Southern Methodist University - Coursework/3 - Spring 2025/DS-7331 Machine Learning 1/1/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('data/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/rhine/Desktop/Southern Methodist University - Coursework/3 - Spring 2025/DS-7331 Machine Learning 1/1/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ML7331/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/rhine/Desktop/Southern Methodist University - Coursework/3 - Spring 2025/DS-7331 Machine Learning 1/1/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv')\n",
    "# df = pd.read_csv(\"C:/Users/rhine/Desktop/Southern Methodist University - Coursework/3 - Spring 2025/DS-7331 Machine Learning 1/1/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a569d-63b8-4667-a343-1c6489746089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a99a94-beda-4bd7-b8c1-8f9c5e6c2196",
   "metadata": {},
   "source": [
    "## 2. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9169d49-3718-40b0-b975-d16b0d8aef17",
   "metadata": {},
   "source": [
    "##### ***Describe the purpose of the data set you selected (i.e. why was this data collected in the first place?).***\n",
    "\n",
    "The dataset comprises 101,766 hospital records of inpatients diagnosed with diabetes who had hospital stays of 1-14 days, collected from 130 U.S. hospitals between 1999 and 2008. While evidence highlights the benefits of preventative and therapeutic interventions in improving outcomes for diabetic patients, diabetes care during hospital stays often appears inconsistent. This dataset was collected to study early readmission rates within 30 days of discharge, aiming to enhance patient outcomes, reduce morbidity and mortality, and lower hospital management costs.\n",
    "\n",
    "**Source:** Strack, B., et al. (2014). *UCI Machine Learning Repository: Diabetes 130-US hospitals for years 1999–2008 Data Set.* Retrieved from [https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008](https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7efb1-b786-4add-9784-60b5ba06eb7f",
   "metadata": {},
   "source": [
    "##### ***Describe how you would define and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset?***\n",
    "\n",
    "The main goal of this dataset is to accurately classify hospital readmissions into three categories: within 30 days, after 30 days, or no readmission. This is important because high readmission rates lead to higher healthcare costs, worse patient outcomes, and added pressure on hospital resources. Identifing variables associated with readmission risk provide valuable information to providers for providing targeted treatments. Identifying these variables to achieve good model performace will indicate that we have extracted useful insights from these data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859e91b-4b5a-41b4-ac62-3998d812a860",
   "metadata": {},
   "source": [
    "##### ***How would you measure the effectiveness of a good prediction algorithm?***\n",
    "\n",
    "A good prediction algorithm for this objective should prioritize Sensitivity, ensuring it correctly identifies patients at risk of early readmission. This is critical for enabling early interventions and preventative treatments. Once a predefined Sensitivity threshold is achieved (e.g. ~80%), the algorithm's Positive Predictive Value (PPV) should then be optimized to minimize false positives, ensuring resources are efficiently allocated and unnecessary treatments are avoided\n",
    "\n",
    "We will also split the data into training and validation sets, using cross-validation to evaluate the algorithm's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12004aa7-2832-4870-b884-cd7371c50384",
   "metadata": {},
   "source": [
    "## 3. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42e204-927d-4a23-bb02-66bca232bcdb",
   "metadata": {},
   "source": [
    "##### ***Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b11dc5-71d7-42f2-93e7-3a50d96fa457",
   "metadata": {},
   "source": [
    "### 3.1 Data Attributes\n",
    "\n",
    "1. **ID Variables**\n",
    "   - **encounter_id**: Unique identifier of an encounter.\n",
    "   - **patient_nbr**: Unique identifier of a patient.  \n",
    "2. **Demographic Variables**\n",
    "   - **race**: Patient's race (Categorical Levels: Caucasian, Asian, African American, Hispanic, other).\n",
    "   - **gender**: Patient's gender (Categorical Levels: male, female, unknown/invalid).\n",
    "   - **age**: Age grouped in 10-year intervals (Categorical Levels: [0-10), [10-20), ..., [90-100)).\n",
    "   - **weight**: Patient's weight in pounds (Categorical).  \n",
    "3. **Admission and Discharge Details**\n",
    "   - **admission_type_id**: Type of admission (Categorical: 9 integer levels representing e.g., Emergency, Urgent, Elective).\n",
    "   - **discharge_disposition_id**: Type of discharge (Categorical: 29 integer levels representing e.g., Discharged to home, Expired).\n",
    "   - **admission_source_id**: Source of admission (Categorical: 21 integer levels representing e.g., Physician Referral, Emergency Room).  \n",
    "4. **Encounter Information**\n",
    "   - **time_in_hospital**: Number of days in the hospital (Continuous).\n",
    "   - **payer_code**: Payer for medical services (Categorical: 18 unique levels, e.g., Blue Cross, Medicare).\n",
    "   - **medical_specialty**: Specialty of admitting physician (Categorical: 73 unique levels, e.g., Cardiology, Internal Medicine).  \n",
    "5. **Medical History and Diagnostics**\n",
    "   - **num_lab_procedures**: Number of lab tests performed (Continuous).\n",
    "   - **num_procedures**: Number of procedures performed (excluding lab tests) (Continuous).\n",
    "   - **num_medications**: Number of distinct medications administered (Continuous).\n",
    "   - **number_outpatient**: Number of outpatient visits in the prior year (Continuous).\n",
    "   - **number_emergency**: Number of emergency visits in the prior year (Continuous).\n",
    "   - **number_inpatient**: Number of inpatient visits in the prior year (Continuous).\n",
    "   - **diag_1**, **diag_2**, **diag_3**: Primary and secondary diagnoses (ICD-9 codes) (Categorical: integer values representing 848, 923, and 954 levels, respectively).\n",
    "   - **number_diagnoses**: Total number of diagnoses recorded (Continuous).  \n",
    "6. **Lab Results**\n",
    "   - **max_glu_serum**: Glucose serum test results (Categorical Levels: >200, >300, normal, none).\n",
    "   - **A1Cresult**: Hemoglobin A1c test results (Categorical Levels: >8, >7, normal, none).  \n",
    "7. **Medications**\n",
    "   - Variables for medications: Indicate whether prescribed and dosage changes (Categorical Levels: up, down, steady, no).\n",
    "   - Drug Classes:\n",
    "     - **Biguanides** reduce glucose production in the liver and improve insulin sensitivity.  \n",
    "       - **metformin**: a first-line treatment for type 2 diabetes\n",
    "     - **Meglitinides** stimulate insulin secretion but act faster and have a shorter duration compared to sulfonylureas. \n",
    "       - **repaglinide**\n",
    "       - **nateglinide**\n",
    "     - **Sulfonylureas 1st Generation** stimulate the pancreas to produce more insulin. These are older drugs with lower potency and a higher risk of side effects, such as hypoglycemia.  \n",
    "       - **chlorpropamide**\n",
    "       - **acetohexamide**\n",
    "       - **tolbutamide**\n",
    "       - **tolazamide**\n",
    "     - **Sulfonylureas 2nd Generation** stimulate the pancreas to produce more insulin. These are newer, more potent drugs with shorter half-lives, reducing the risk of hypoglycemia.  \n",
    "       - **glimepiride**\n",
    "       - **glipizide**\n",
    "       - **glyburide**\n",
    "     - **Thiazolidinediones (TZDs)** improve insulin sensitivity in muscle and fat tissue. They can have serious side effects, including weight gain and fluid retention. (They have been withdrawn in many markets due to evidence of liver toxicity.)\n",
    "       - **pioglitazone**\n",
    "       - **rosiglitazone**\n",
    "       - **troglitazone**\n",
    "     - **Alpha-Glucosidase Inhibitors** delay carbohydrate digestion and absorption in the intestines, reducing post-meal blood sugar spikes.\n",
    "       - **acarbose**\n",
    "       - **miglitol**\n",
    "     - **DPP-4 Inhibitors** enhance incretin hormone levels to increase insulin release and decrease glucagon.\n",
    "       - **citoglipton**: An unknown drug and potential error in the dataset; it may be a misspelling of *sitagliptin*.\n",
    "     - **Insulin and Combinations**: Combination drugs combine two medications to simplify treatment and target multiple mechanisms for blood sugar control.\n",
    "       - **insulin** directly supplements or replaces natural insulin in the body, essential for type 1 and advanced type 2 diabetes management.\n",
    "       - **glyburide-metformin**\n",
    "       - **glipizide-metformin**\n",
    "       - **glimepiride-pioglitazone**\n",
    "       - **metformin-rosiglitazone**\n",
    "       - **metformin-pioglitazone**\n",
    "     - **Non-Diabetes Medications**\n",
    "       - **examide** is a diuretic primarily prescribed for cardiovascular or renal issues, not blood sugar control.  \n",
    "8. **Medication Outcome Variables**\n",
    "   - **change**: Indicates if diabetic medications were changed during the encounter (Categorical Levels: change, no change).\n",
    "   - **diabetesMed**: Indicates if diabetic medications were prescribed (Categorical Levels: yes, no).\n",
    "9. **Target Variable**\n",
    "   - **readmitted**: Readmission status (Target Classes: <30, >30, No).  \n",
    "\n",
    "---\n",
    "\n",
    "##### Metadata Categories\n",
    "\n",
    "1. **admission_type_id**\n",
    "   - 1: Emergency; 2: Urgent; 3: Elective; 4: Newborn; 5: Not Available; 6: NULL; 7: Trauma Center; 8: Not Mapped    \n",
    "2. **discharge_disposition_id**\n",
    "   - 1: Discharged to home; 2: Discharged/transferred to another short-term hospital; 3: Discharged/transferred to SNF; 4: Discharged/transferred to ICF; 5: Discharged/transferred to another type of inpatient care institution; 6: Discharged/transferred to home with home health service; 7: Left AMA; 8: Discharged/transferred to home under care of Home IV provider; 9: Admitted as an inpatient to this hospital; 10: Neonate discharged to another hospital for neonatal aftercare; 11: Expired; 12: Still patient or expected to return for outpatient services; 13: Hospice / home; 14: Hospice / medical facility; 15: Discharged/transferred within this institution to Medicare approved swing bed; 16: Discharged/transferred/referred to another institution for outpatient services; 17: Discharged/transferred/referred to this institution for outpatient services; 18: NULL; 19: Expired at home (Medicaid only, hospice); 20: Expired in a medical facility (Medicaid only, hospice); 21: Expired, place unknown (Medicaid only, hospice); 22: Discharged/transferred to another rehab facility including rehab units of a hospital; 23: Discharged/transferred to a long-term care hospital; 24: Discharged/transferred to a nursing facility certified under Medicaid but not certified under Medicare; 25: Not Mapped; 26: Unknown/Invalid; 27: Discharged/transferred to a federal health care facility; 28: Discharged/transferred/referred to a psychiatric hospital of psychiatric distinct part unit of a hospital; 29: Discharged/transferred to a Critical Access Hospital (CAH); 30: Discharged/transferred to another type of health care institution not defined elsewhere \n",
    "3. **admission_source_id**\n",
    "   - 1: Physician Referral; 2: Clinic Referral; 3: HMO Referral; 4: Transfer from a hospital; 5: Transfer from a Skilled Nursing Facility (SNF); 6: Transfer from another health care facility; 7: Emergency Room; 8: Court/Law Enforcement; 9: Not Available; 10: Transfer from critical access hospital; 11: Normal Delivery; 12: Premature Delivery; 13: Sick Baby; 14: Extramural Birth; 15: Not Available; 17: NULL; 18: Transfer from Another Home Health Agency; 19: Readmission to Same Home Health Agency; 20: Not Mapped; 21: Unknown/Invalid; 22: Transfer from hospital inpatient/same facility resulting in a separate claim; 23: Born inside this hospital; 24: Born outside this hospital; 25: Transfer from Ambulatory Surgery Center; 26: Transfer from Hospice  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636f4cf-67a6-4445-97e9-f469ed2eef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the variable metadata in a more readable table format\n",
    "# metadata = pd.read_csv('data/diabetes+130-us+hospitals+for+years+1999-2008/IDS_mapping.csv')\n",
    "# print(metadata.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d39b1-2c43-4884-b8c4-1c03925a7328",
   "metadata": {},
   "source": [
    "### 3.2 Data Cleaning\n",
    "\n",
    "##### ***Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems?***\n",
    "\n",
    "#### 3.2.1 Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059dc95-376d-4e6d-bb40-c523b1b26e60",
   "metadata": {},
   "source": [
    "##### **A. Missing Data Summary Statistics and Visualization**\n",
    "\n",
    "This identifies columns with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e04e2-a713-4994-90ad-f3ec6fe142a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the NaNs\n",
    "nan_sums = df.isna().sum()\n",
    "nan_sums_nonzero = nan_sums[nan_sums != 0]\n",
    "nan_sums_nonzero\n",
    "\n",
    "# Calculate percentages\n",
    "nan_percentages = (nan_sums_nonzero / len(df)) * 100\n",
    "\n",
    "# Create a DataFrame of NaN counts and percentages\n",
    "nan_summary = pd.DataFrame({\n",
    "    'NaN Count': nan_sums_nonzero,\n",
    "    'NaN Percentage': nan_percentages\n",
    "}).sort_values(by='NaN Count', ascending=False)\n",
    "nan_sums_nonzero\n",
    "\n",
    "nan_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703b69f-7040-4644-8efa-77244e1adb83",
   "metadata": {},
   "source": [
    "This identifies columns with '?' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4887389-12c0-4d1e-98d7-13246a5493e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find `?` values and decide how to deal with them\n",
    "questionmark_sums = (df == '?').sum()\n",
    "questionmark_sums_nonzero = questionmark_sums[questionmark_sums != 0]\n",
    "questionmark_sums_nonzero\n",
    "\n",
    "# Calculate percentages\n",
    "questionmark_percentages = (questionmark_sums_nonzero / len(df)) * 100\n",
    "\n",
    "# Create a DataFrame of NaN counts and percentages\n",
    "questionmark_summary = pd.DataFrame({\n",
    "    '? Count': questionmark_sums_nonzero,\n",
    "    '? Percentage': questionmark_percentages\n",
    "}).sort_values(by='? Count', ascending=False)\n",
    "questionmark_sums_nonzero\n",
    "\n",
    "questionmark_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c93c75-888d-45a8-92ff-a54fc9e002e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `?` with NaN for now\n",
    "df_clean = df.copy()\n",
    "df_clean = df.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e863e1-e605-4685-9672-ee2c94285d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many missing values are in each weight class?\n",
    "\n",
    "# Define the order for weight classes\n",
    "weight_order = [\n",
    "    \"[0-25)\", \"[25-50)\", \"[50-75)\", \"[75-100)\", \"[100-125)\", \n",
    "    \"[125-150)\", \"[150-175)\", \"[175-200)\", \">200\", np.nan\n",
    "]\n",
    "\n",
    "weight_counts = df_clean['weight'].value_counts(dropna=False)\n",
    "\n",
    "# Reindex the counts based on the custom order\n",
    "sorted_weight_counts = weight_counts.reindex(weight_order)\n",
    "sorted_weight_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59e2da-b80f-418e-ab39-9472d6e8bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at age. The low weights will likely be young patients.\n",
    "\n",
    "df_clean['age'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224883c5-e610-443d-b14e-094529d030fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "under_50_lbs = ((df_clean['weight'] == '[0-25)') | (df_clean['weight'] == '[25-50)')).sum()\n",
    "under_10_yrs = (df_clean['age'] == '[0-10)').sum()\n",
    "\n",
    "print(f'There are {under_50_lbs} patients under 50 lbs and {under_10_yrs} patients under 10 years old.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7ed25-b065-4c7a-882e-b2e1a2883ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look closer at max_glu_serum representing glucose serum test results\n",
    "# One of the categorical levels is `none`. This test is not conducted on most patients.\n",
    "\n",
    "df_clean['max_glu_serum'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bffb1-e2a8-4e95-9563-8fdf8843d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at A1Cresult\n",
    "# One of the categorical levels is `none`. This test is not conducted on most patients.\n",
    "\n",
    "df_clean['A1Cresult'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9b2c8-2e64-4e8b-a62b-6b08bd0a7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df_clean['medical_specialty'].nunique()} unique categories in the `medical_specialty` variable.')\n",
    "med_specialty_categories = df_clean['medical_specialty'].unique()\n",
    "med_specialty_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8c444-5f69-4b0c-9830-2776f21649ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_clean['medical_specialty'].value_counts(dropna=False)[df_clean['medical_specialty'].value_counts(dropna=False)>1000]\n",
    "percentages = (value_counts / len(df_clean['medical_specialty'])) * 100\n",
    "med_specialty_df = pd.DataFrame({\n",
    "    'Category': value_counts.index,\n",
    "    'Count': value_counts.values,\n",
    "    'Percentage (%)': percentages.values,\n",
    "})\n",
    "med_specialty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65aec9-572f-4211-9a2e-4208d6885fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_clean['payer_code'].value_counts(dropna=False)\n",
    "percentages = (value_counts / len(df_clean['payer_code'])) * 100\n",
    "payer_df = pd.DataFrame({\n",
    "    'Category': value_counts.index,\n",
    "    'Count': value_counts.values,\n",
    "    'Percentage (%)': percentages.values,\n",
    "})\n",
    "payer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40eeadd-138b-44fb-be25-bb4916d8c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_clean['race'].value_counts(dropna=False)\n",
    "percentages = (value_counts / len(df_clean['race'])) * 100\n",
    "race_df = pd.DataFrame({\n",
    "    'Category': value_counts.index,\n",
    "    'Count': value_counts.values,\n",
    "    'Percentage (%)': percentages.values,\n",
    "})\n",
    "race_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0af1e7-44a2-4394-8bbb-3d6db2e03b61",
   "metadata": {},
   "source": [
    "It seems reasonable that not every patient would have 3 diagnoses. Only 21 are missing diag_1. Are those also missing diag_2 and diag_3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74d7ac-ab92-4d58-85ae-41d78dc3e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes where 'diag_1' was '?' (now NaN)\n",
    "indexes_with_diag_1_questionable = df_clean[df_clean['diag_1'].isna()].index.tolist()\n",
    "\n",
    "# Look at the rows where diag_1 is '?'\n",
    "rows_with_diag_1_questionable = df_clean.loc[indexes_with_diag_1_questionable, ['diag_1', 'diag_2', 'diag_3']]\n",
    "\n",
    "rows_with_diag_1_questionable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b52eea-2b8c-4950-94f1-7deabcf1cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_clean['diag_3'].value_counts(dropna=False)[df_clean['diag_3'].value_counts(dropna=False)>1400]\n",
    "percentages = (value_counts / len(df_clean['diag_3'])) * 100\n",
    "diag3_df = pd.DataFrame({\n",
    "    'Category': value_counts.index,\n",
    "    'Count': value_counts.values,\n",
    "    'Percentage (%)': percentages.values,\n",
    "})\n",
    "diag3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc48a4d-0365-4456-96cd-f49a0d8f4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_variables = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "null_values = [6, 18, 17]\n",
    "\n",
    "admin_null_df = pd.DataFrame({\n",
    "    'Variable': pd.Series(dtype='str'),\n",
    "    'Null_Value': pd.Series(dtype='int'),\n",
    "    'Count': pd.Series(dtype='int'),\n",
    "    'Percentage (%)': pd.Series(dtype='float')\n",
    "})\n",
    "\n",
    "for var, null_val in zip(admin_variables, null_values):\n",
    "    # Get value counts for the current variable\n",
    "    value_counts = df_clean[var].value_counts(dropna=False)\n",
    "    \n",
    "    # Extract the count and calculate percentage for the specific \"null\" value\n",
    "    count = value_counts.get(null_val, 0)  # Use .get() to safely retrieve the count\n",
    "    percentage = (count / len(df_clean[var])) * 100\n",
    "\n",
    "    # Create a temporary DataFrame for this variable\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Variable': [var],\n",
    "        'Null_Value': [null_val],\n",
    "        'Count': [count],\n",
    "        'Percentage (%)': [percentage]\n",
    "    })\n",
    "\n",
    "    # Concatenate the temporary DataFrame with the main one\n",
    "    admin_null_df = pd.concat([admin_null_df, temp_df], ignore_index=True)\n",
    "\n",
    "admin_null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0938d",
   "metadata": {},
   "source": [
    "Additionally, we need to be wary of any columns with zero variance. Columns that share the same value for every observation offer no explanatory insights for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify zero variance columns\n",
    "zero_variance_cols = [col for col in df_clean.columns if df_clean[col].nunique() <= 1]\n",
    "zero_variance_df = df_clean.loc[:, zero_variance_cols]\n",
    "zero_variance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb52200-b1d1-41eb-b40f-7f201f78ccc2",
   "metadata": {},
   "source": [
    "Variables with missing values (either `NaN` or `?`) include `weight`, `max_glu_serum`, `A1Cresult`, `medical_specialty`, `payer_code`, `race`, `diag_3`, `diag_2`, and `diag_1`. There are also integer levels representing `NULL` in `admission_type_id`, `discharge_disposition_id`, and `admission_source_id`. Finally, there are two columns with zero variance: `examide` and `citoglipton`.\n",
    "\n",
    "##### Strategies for dealing with missing data:\n",
    "\n",
    "- It seems that for features with a high percentage of missing values (e.g. weight, max_glu_serum, A1Cresult), we should remove these features from our analysis.   \n",
    "- However, is there a way to examine the rows with these values to determine their predictive power? It may be that prediction can be improved significantly with these. Part of the conclusion and recommendation may be for hospitals to include these test for all diabetes patients.\n",
    "  - Check correlations between these variables and the target variable for rows where data is available.\n",
    "  - Use a random forest model to evaluate feature importance to see if these features contribute significantly to prediction when included.\n",
    "  - Investigate differences in the target variable for patients with and without these values.\n",
    "  - Consider that these may test for long term markers rather than parameters critical for acute care. Also, they may be done at routine doctors appointments and they are avoided during acute treatment to avoid duplication.    \n",
    "- It would be interesting to investigate if a certain payer code or medical specialty for which we have records are associated with lower rates of treatment or higher rates of re-admission.\n",
    "  - Use bar plots or heat maps to look into how payer codes and medical specialties are distributed across patients with different outcomes (e.g. re-admission rates).\n",
    "  - Certain payer codes or specialties may correlate with lower rates of tests or treatments, which could indicate disparities in care.\n",
    "  - Consider if it is better to recode a '?', the value in the original dataset, with NaN or a different category like 'Unknown'.  \n",
    "- Features with < 3% missing values (e.g. race, diag_3, diag_2, diag_1) imputation seems like a viable option, either using the mode or something more sophisticated like a multi-class classification technique.\n",
    "  - It is also plausible that some patients only have one or two diagnoses so that is why diag_2 and diag_3 are missing.\n",
    "- There are also levels coded as integers representing `NULL` in admission_type_id (6), discharge_disposition_id (18), and admission_source_id (17). It is unclear what this means. \n",
    "- Regarding the zero variance columns, there's not much we can do except drop these columns and continue the analysis without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226b7b6-81b2-4ba6-887f-467663334028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# Visualize missing data as a heatmap\n",
    "msno.heatmap(df_clean, fontsize=10, figsize=(8, 6))\n",
    "# plt.savefig(f'plots/missing_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643834d-dccb-4c0f-9123-ab85b403ab07",
   "metadata": {},
   "source": [
    "The heatmap displays correlations in the patterns of missingness between variables. A positive correlation of 0.4 between diag_2 and diag_3 indicates that if a record is missing a value for diag_2, there is an increased likelihood that it is also missing a value for diag_3. This relationship suggests that if diag_2 is missing, we may be able to use diag_3 (if available) to inform our imputation strategy.  \n",
    "The lack of strong correlations between most other variables suggests their missingness is likely independent of one another. However, negative correlations (e.g., -0.1 between medical_specialty and max_glu_serum) indicate a weak inverse relationship, meaning that when one column has a missing value, the other is slightly less likely to be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbc685-91e0-475e-8b7c-a3d8e39acde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data as a matrix\n",
    "msno.matrix(df_clean.loc[:, df_clean.isnull().any()])\n",
    "# plt.savefig(f'plots/missing_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06873da-3c41-4bea-b11f-89ee3528e6cf",
   "metadata": {},
   "source": [
    "This bar graph displays records across the full dataset, with missing values shown in white. There doesn’t appear to be a strong indication that specific rows are disproportionately affected by missingness. Patterns of missing values seem fairly distributed across rows without obvious clustering. This suggests that while variable-level missingness is significant, row-level missingness may not require targeted removal or specific handling.The dendrogram provides insights into relationships between variables based on their missingness patterns. Diagnosis codes (diag_1, diag_2, diag_3) cluster together, indicating shared trends in missing values, which may reflect relationships in the data collection process. Variables like max_glu_serum and weight also cluster, though this may be less meaningful due to the high proportion of missing data in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12b739-5c9e-49c7-ab5f-8d6389c96ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dendrogram for missingness correlations\n",
    "msno.dendrogram(df_clean.loc[:, df_clean.isnull().any()])\n",
    "# plt.savefig(f'plots/missing_dendrogram.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f47830-4a52-4b43-adf9-d84bfe03d749",
   "metadata": {},
   "source": [
    "The dendrogram provides insights into relationships between variables based on their missingness patterns. Diagnosis codes (diag_1, diag_2, diag_3) cluster together, indicating shared trends in missing values, which may reflect relationships in the data collection process. Variables like max_glu_serum and weight also cluster, though this may be less meaningful due to the high proportion of missing data in these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37958ba-f7ed-4cad-a831-ba48861a4da5",
   "metadata": {},
   "source": [
    "##### **B. Decisions on handling the missing data for further processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3233a-6414-4ca5-b0d1-6d85a1bb59dc",
   "metadata": {},
   "source": [
    "There isn't an Unknown category in medical_specialty, payer_code, or race. It seems reasonable to make the NaN ('?') values an Unknown category rather than a NaN. Also, it seems plausible that not every patient would have more than one diagnosis code. Converting NaN ('?') to Unknown/None seems reasonable for the diagnosis codes. The data set documentation describes a 'none' category for both max_glu_serum and A1Cresult. Making the assumption that the NaN values are the values in the 'none' category and 'none' likely represents patients that did not receive this test, NaN values were replaced with Untested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea220b25-0ec3-4263-988a-3ff9bf701374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN ('?') with 'Unknown' in the specified columns\n",
    "columns_to_update_1 = ['medical_specialty', 'payer_code', 'race']\n",
    "df_clean[columns_to_update_1] = df_clean[columns_to_update_1].replace(np.nan, 'Unknown')\n",
    "\n",
    "# Replace NaN ('?') with 'Unknown' in the specified columns\n",
    "columns_to_update_2 = ['diag_1', 'diag_2', 'diag_3']\n",
    "df_clean[columns_to_update_2] = df_clean[columns_to_update_2].replace(np.nan, 'Unknown/None')\n",
    "\n",
    "# Replace NaN with 'Untested' in the specified columns\n",
    "columns_to_update_3 = ['max_glu_serum', 'A1Cresult']\n",
    "df_clean[columns_to_update_3] = df_clean[columns_to_update_3].replace(np.nan, 'Untested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67025e6-5c68-4c0f-9830-57d2731f176c",
   "metadata": {},
   "source": [
    "#### 3.2.2 Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d918f2-eaa2-4222-b8b8-6cf11f258ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_count = df_clean.duplicated().sum()\n",
    "print(f'The dataset has {duplicate_count} duplicate records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de86e2-e6f0-4cbb-9d89-29c742ab53cf",
   "metadata": {},
   "source": [
    "#### 3.2.3 Remove ID variable: 'encounter_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1471661-da4d-41b8-90ca-70f881d27448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that encounter_id is unique for each record.\n",
    "df_clean['encounter_id'].nunique() == len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9c355-e1bb-4e69-a08b-9fd377160a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'encounter_id' in df_clean:\n",
    "    del df_clean['encounter_id']\n",
    "\n",
    "# if 'patient_nbr' in df_clean:\n",
    "#     del df_clean['patient_nbr']\n",
    "\n",
    "print( df_clean.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a21274-5d9d-4bdb-b842-fc5a6aa0fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables `patient_nbr`, `admission_type_id`, `discharge_disposition_id`, `admission_source_id`\n",
    "# from integer to object datatype.\n",
    "categoricalInt_cols = ['patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n",
    "df_clean[categoricalInt_cols] = df_clean[categoricalInt_cols].astype('category')\n",
    "df_clean[['patient_nbr', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id']].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eb730-9469-44c3-8a48-11ed0e79110f",
   "metadata": {},
   "source": [
    "### 3.3 Statistical summaries of the features\n",
    "\n",
    "##### ***Give simple, appropriate statistics (range, mode, mean, median, variance, counts, etc.) for the most important attributes and describe what they mean or if you found something interesting. Note: You can also use data from other sources for comparison. Explain the significance of the statistics run and why they are meaningful.***\n",
    "\n",
    "#### 3.3.1 Numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f33761",
   "metadata": {},
   "source": [
    "Statistical Summaries of Numerical Attributes:\n",
    "-   Count: This is the raw count of observations for a particular attribute in the dataset\n",
    "-   Mean: The average value of the observations for a particular attribute.\n",
    "-   Std: The standard deviation, measuring the spread of values among observations.\n",
    "-   Variance: The average of the squared differences from the mean, showing variability.\n",
    "-   Min: The smallest value among the observations for a particular attribute.\n",
    "-   25%: The first quartile, representing the value below which 25% of the data falls.\n",
    "-   Median: The middle value when observations are sorted, dividing the dataset into two equal halves.\n",
    "-   75: The third quartile, representing the value below which 75% of the data falls.\n",
    "-   Max: The largest value among the observations for a particular attribute.\n",
    "-   Mode: This is the most commonly recurring value among observations for a particular attribute\n",
    "-   Range: The difference between the maximum and minimum values, showing the span of data\n",
    "-   Kurtosis: A measure of how heavily the tails of the distribution differ compared to a normal distribution, showing the extent of the outliers.\n",
    "    -   Kurstosis ~= 3, Tails are neither heavy nor light\n",
    "    -   Kurtosis > 3, Heavy tails and a sharper peak\n",
    "    -   Kurtosis < 3, Lighter tails and a flatter peak\n",
    "-   Skew: A measure of asymmetry in the distribution of the data. \n",
    "    -   Skew < 0 indicates Negative/Left-Skew\n",
    "    -   Skew > 0 indicates Positive/Right-Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f2dae-fb72-40ec-808f-8de04bd8c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the numerical features\n",
    "\n",
    "# Calculate basic statistics\n",
    "desc = df_clean.describe()  # Includes count, mean, std, 5-number summary\n",
    "modes_num = df_clean.mode(numeric_only=True).iloc[0]  # Get the first mode for each column\n",
    "variances = df_clean.var(numeric_only=True)\n",
    "ranges = df_clean.max(numeric_only=True) - df_clean.min(numeric_only=True)\n",
    "kurt = df_clean.kurt(numeric_only = True)\n",
    "skew = df_clean.skew(numeric_only = True)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "num_stats_df = pd.DataFrame({\n",
    "    'Count': desc.loc['count'],\n",
    "    'Mean': desc.loc['mean'],\n",
    "    'Std': desc.loc['std'],\n",
    "    'Min': desc.loc['min'],\n",
    "    '25%': desc.loc['25%'],\n",
    "    'Median': desc.loc['50%'],\n",
    "    '75%': desc.loc['75%'],\n",
    "    'Max': desc.loc['max'],\n",
    "    'Range': ranges,\n",
    "    'Variance': variances,\n",
    "    'Mode': modes_num,\n",
    "    'Kurtosis': kurt,\n",
    "    'Skew': skew\n",
    "})\n",
    "\n",
    "# Rearrange and clean up for better presentation\n",
    "num_stats_df = num_stats_df[['Count', 'Mean', 'Std', 'Variance', 'Min', '25%', 'Median', '75%', 'Max', 'Range', 'Mode', 'Kurtosis', 'Skew']]\n",
    "num_stats_df.reset_index(inplace=True)  # Optional: Keep variable names as a column\n",
    "num_stats_df.rename(columns={'index': 'Variable'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "num_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0c8aa-4c12-401d-bc70-6e17564197d3",
   "metadata": {},
   "source": [
    "We computed the following summary statistics for the numerical variables: count, mean, standard deviation, variance, five-number summary (minimum and maximum, 25th and 75th percentile, and median), range, and mode. These tell us how many records there are and give us a sense of the distribution of the data. We have the measures of center (mean, median and mode), as well as measures of spread (standard deviation and variance). We also can see the full range of the data and and where the middle 50% of the data lie.  \n",
    "\n",
    "For many of these features the mean and median (and also mostly the mode) are similar values. The distribution appear to be right skewed in many cases by high outliers. The exception to that seems to be number of lab procedures, which seems to be more normally distributed.  \n",
    "\n",
    "Additionally, in combination with skewness, several variables have a max-to-min ratio much greater than 10 (e.g. num_lab_procedures, num_medications, number_outpatient, number_emergency, number_inpatient), suggesting that a log transformation may be helpful.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04533f5-a8d0-4888-9efa-d86ab20869b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of the categorical features\n",
    "\n",
    "# Calculate basic statistics\n",
    "desc_cat = df_clean.describe(include=['object', 'category'])  # Includes count, unique, top, freq\n",
    "top_percentage = (desc_cat.loc['freq'] / desc_cat.loc['count'] * 100).round(2)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "cat_stats_df = pd.DataFrame({\n",
    "    'Count': desc_cat.loc['count'],\n",
    "    'Unique': desc_cat.loc['unique'],\n",
    "    'Top': desc_cat.loc['top'],\n",
    "    'Freq': desc_cat.loc['freq'],\n",
    "    'Top %': top_percentage\n",
    "})\n",
    "\n",
    "# Rearrange and clean up for better presentation\n",
    "cat_stats_df.reset_index(inplace=True)  # Optional: Keep variable names as a column\n",
    "cat_stats_df.rename(columns={'index': 'Variable'}, inplace=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "cat_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a62a3-49fe-4231-af0c-faf896f82cd4",
   "metadata": {},
   "source": [
    "For the categorical variables (including the target), we computed the following summary statistics: count, number of unique values (levels), `Top` - the level that occurs most frequently, `Freq` - the number of occurences of the top level, and the percentage of the values within that level.  \n",
    "This indicates that for many features, nearly all of the values fall within one category (e.g. over 99% of patients do not take 15 out of the 23 drugs in the dataset). We also can see that there are repeated patients in the dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc5a3d-7f2f-4765-a546-e9607190d810",
   "metadata": {},
   "source": [
    "#### 3.3.2 Categorical features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd08f2-843c-42cf-be31-9a4bc4c086da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the proportion of each response class.\n",
    "print(df_clean['readmitted'].value_counts()/df_clean['readmitted'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d50f8-c0c2-49fe-9cc3-ea588f1f3f5e",
   "metadata": {},
   "source": [
    "The response class is unbalanced with 54% patient records not being readmitted, 35% being readmitted in greater than 30 days, and 11% being readmitted in less than 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bf818-3079-45e3-8e0c-3f195b6c8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many patients are have more than one encounter.\n",
    "\n",
    "patient_counts = df_clean['patient_nbr'].value_counts(dropna=False)\n",
    "repeated_patients = patient_counts[patient_counts > 1]\n",
    "num_repeated_patients = len(repeated_patients)\n",
    "print(f\"Number of patients with more than one record: {num_repeated_patients}\")\n",
    "\n",
    "repeated_patients.min(), repeated_patients.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6a3d0-c7f9-4309-ba5d-4ea197106a3f",
   "metadata": {},
   "source": [
    "patient_nbr is not unique for each record in the dataset indicating that there are repeated measures. 16,773 patients have between 2 and 40 encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b83a44-5f0f-41d0-a128-4d11c9a06af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that values in 'examide' and 'citoglipton' are 100% 'No' across the full dataset\n",
    "print(df_clean['examide'].value_counts())\n",
    "print(df_clean['citoglipton'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e51ca4-2f63-4e20-bd01-611c543d68ed",
   "metadata": {},
   "source": [
    "We should be able to eliminate these variables, since they aren't offering any insight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2d17d-d7ec-4b2e-9f8d-6a5d59082f7b",
   "metadata": {},
   "source": [
    "#### 3.3.3 Remove zero variance and high missing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32ce81-cc5b-476f-ba7b-114ec32e5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove examide and citoglipton from the dataset.\n",
    "df_clean = df_clean.drop(columns=['examide', 'citoglipton'])\n",
    "\n",
    "df_clean = df_clean.drop(columns=['weight'])\n",
    "\n",
    "print( df_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9cbde-c65b-42ac-80b2-62a0e0ed8e1c",
   "metadata": {},
   "source": [
    "#### 3.3.4 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009721e-d66f-48b6-bc7e-a02ce2a41f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR and outlier bounds\n",
    "num_stats_df['IQR'] = num_stats_df['75%'] - num_stats_df['25%']  # Interquartile Range\n",
    "num_stats_df['Lower_IQR_Bound'] = num_stats_df['25%'] - 1.5 * num_stats_df['IQR']\n",
    "num_stats_df['Upper_IQR_Bound'] = num_stats_df['75%'] + 1.5 * num_stats_df['IQR']\n",
    "\n",
    "outliers = {} # Empty dictionary to store outliers for each variable\n",
    "total_rows = len(df_clean)\n",
    "\n",
    "for var in df_clean.select_dtypes(include='number').columns:\n",
    "    # Get lower and upper bounds for the variable\n",
    "    lower_bound = num_stats_df.loc[num_stats_df['Variable'] == var, 'Lower_IQR_Bound'].values[0]\n",
    "    upper_bound = num_stats_df.loc[num_stats_df['Variable'] == var, 'Upper_IQR_Bound'].values[0]\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers[var] = df_clean[(df_clean[var] < lower_bound) | (df_clean[var] > upper_bound)][var]\n",
    "\n",
    "# Summarize outliers\n",
    "outlier_counts = {var: len(outlier_values) for var, outlier_values in outliers.items()}\n",
    "outlier_proportions = {var: round((count / total_rows) * 100, 2) for var, count in outlier_counts.items()}  # Convert to percentage and round\n",
    "\n",
    "# Create a summary DataFrame\n",
    "outliers_summary_df = pd.DataFrame({\n",
    "    'Variable': outlier_counts.keys(),\n",
    "    'Outlier Count': outlier_counts.values(),\n",
    "    'Outlier Proportion (%)': outlier_proportions.values()  # Updated to percentage\n",
    "})\n",
    "\n",
    "outliers_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73a69a-e0a7-44b6-b3b5-0b4e0a3802a5",
   "metadata": {},
   "source": [
    "Suspected outliers were identified using the interquartile range (IQR) method: values below the 25th percentile minus 1.5 times the IQR or above the 75th percentile plus 1.5 times the IQR were classified as outliers. Variables related to hospital visits (e.g. number_outpatient, number_emergency, number_inpatient) exhibited a higher proportion of outliers, with more than 5% of observations flagged. These variables had low mean, median, and mode values, but a small subset of patients had disproportionately high numbers of visits.  \n",
    "Other numeric variables had fewer than 5% of observations defined as outliers. It may be worth examining potential correlations among the hospital visit variables, as certain patients could be more prone to frequent hospital visits and associated interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variable_grid(n_variables, n_cols=3, figsize=(16, 4)):\n",
    "    \"\"\"\n",
    "    Creates a dynamic grid layout for subplots.\n",
    "\n",
    "    Parameters:\n",
    "    - n_variables: Total number of variables to visualize.\n",
    "    - n_cols: Number of columns in the subplot grid.\n",
    "    - figsize: Tuple representing the figure size per row.\n",
    "\n",
    "    Returns:\n",
    "    - fig, axes: Matplotlib figure and flattened axes array.\n",
    "    \"\"\"\n",
    "    # Calculate the number of rows needed\n",
    "    n_rows = (n_variables + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(figsize[0], figsize[1] * n_rows))\n",
    "    axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "\n",
    "    # Turn off unused subplots\n",
    "    for ax in axes[n_variables:]:\n",
    "        ax.axis(\"off\")  # Hide unused axes\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "numerical_df = df_clean.select_dtypes(include='number')\n",
    "\n",
    "def improved_qqplot(numerical_df, axes):\n",
    "    \"\"\"\n",
    "    Enhanced Q-Q plot with reference line, outlier highlighting, and summary statistics.\n",
    "    \"\"\"\n",
    "    # Perform Q-Q plot\n",
    "    stats.probplot(numerical_df, dist = \"norm\", plot = axes)\n",
    "\n",
    "    # Add title and labels\n",
    "    axes.set_title(f\"Q-Q: {column}\", fontsize=12)    \n",
    "    axes.set_xlabel(\"Theoretical Quantiles\", fontsize=10)\n",
    "    axes.set_ylabel(\"Sample Quantiles\", fontsize=10)\n",
    "\n",
    "    # Annotate summary statistics\n",
    "    skewness = stats.skew(numerical_df, nan_policy=\"omit\")\n",
    "    kurtosis = stats.kurtosis(numerical_df, nan_policy=\"omit\")\n",
    "    axes.text(0.05, 0.9, f\"Skewness: {skewness:.2f}\\nKurtosis: {kurtosis:.2f}\",\n",
    "            transform=axes.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "fig, axes = create_variable_grid(len(numerical_df.columns))\n",
    "for i, column in enumerate(numerical_df.columns):\n",
    "    improved_qqplot(numerical_df[column].dropna(), axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af9bef",
   "metadata": {},
   "source": [
    "Q-Q Plots:\n",
    "-   time_in_hospital:\n",
    "    -   Skewness: 1.13: Indicating a moderate right-skew, suggesting values tend to concentrate on the left side of the distribution.\n",
    "    -   Kurtosis: 0.85: The kurtosis indicates that the distribution is slightly more peaked than a normal distribution\n",
    "    -   Overall: The distribution is slightly asymmetric with a right skew and relatively light tails.\n",
    "-   num_lab_procedures: \n",
    "    -   Skewness: -0.24: A skewness of -0.24 indicates a slight left skew; but it's very close to zero, suggesting near symmetry.\n",
    "    -   Kurtosis: -0.25: The kurtosis indicates that the distribution is slightly flatter than a normal distribution.\n",
    "    -   Overall: The distribution is nearly symmetric with a slight left skew and flat tails, indicating a lack of extreme outliers.\n",
    "-   num_procedures: \n",
    "    -   Skewness: 1.32: The positive skewness indicates a moderate right skew, with a tail extending towards higher values.\n",
    "    -   Kurtosis: 0.86:Light tails, suggesting fewer extreme outliers than a normal distribution\n",
    "    -   Overall: The distribution is asymmetric with a noticeable right skew and relatively light tails, indicating a concentration of values toward the lower end.\n",
    "-   num_medications: \n",
    "    -   Skewness: 1.33: Moderately positively skewed, indicating a right tail in the distribution\n",
    "    -   Kurtosis: 3.47: High kurtosis, suggesting the presence of more extreme outliers than a normal distribution\n",
    "    -   Overall:  The distribution is asymmetric with a significant right skew and heavy tails, indicating a concentration of lower values with some extreme high values.\n",
    "-   number_outpatient: \n",
    "    -   Skewness: 8.83: Very high skewness (8.83), indicating a significant right skew. The data has many low values and a few extreme high values.\n",
    "    -   Kurtosis: 147.90: The extremely high kurtosis (147.90) suggests that the distribution is heavily tailed, with many extreme outliers.\n",
    "    -   Overall: The distribution is highly asymmetric, dominated by a heavy right tail and extreme outliers, with most values concentrated near zero.\n",
    "-   number_emergency: \n",
    "    -   Skewness: 22.86: The skewness value of 22.86 is extraordinarily high, indicating a severe right skew. This suggests the presence of extreme values far above the rest of the data.\n",
    "    -   Kurtosis: 1191.63: The kurtosis is extremely high (1191.63), indicating that the distribution has very heavy tails and many extreme outliers.\n",
    "    -   Overall: The distribution is highly asymmetric, with most values near zero and a small number of extreme high values creating a very heavy right tail.\n",
    "-   number_inpatient: \n",
    "    -   Skewness: 3.61: This value would indicate that there's a strong right skew.\n",
    "    -   Kurtosis: 20.72: The kurtosis is very high, suggesting that the distribution has heavy tails with extreme outliers.\n",
    "    -   Overall: The distribution is asymmetric, with a noticeable right skew and heavy tails caused by extreme high values.\n",
    "-   number_diagnoses: \n",
    "    -   Skewness: -0.88: Moderately negatively skewed, indicating a left tail in the distribution\n",
    "    -   Kurtosis: -0.08: Nearly normal kurtosis, suggesting a shape close to a normal distribution\n",
    "    -   Overall: The distribution is slightly asymmetric with a left skew and relatively normal tails, indicating a mild concentration of lower values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ddc02",
   "metadata": {},
   "source": [
    "Let's identify and plot the outliers using the IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers_iqr(numerical_df, multiplier=1.5, axes=None):\n",
    "    \"\"\"\n",
    "    Identifies and visualizes outliers in numerical columns using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "        numerical_df (pd.DataFrame): DataFrame with numerical columns.\n",
    "        multiplier (float): The multiplier for the IQR to define outliers.\n",
    "        axes (list): List of Matplotlib axes to plot on.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for i, column in enumerate(numerical_df.columns):\n",
    "        Q1 = numerical_df[column].quantile(0.25)\n",
    "        Q3 = numerical_df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        outliers = numerical_df[(numerical_df[column] < lower_bound) | (numerical_df[column] > upper_bound)]\n",
    "\n",
    "        sns.scatterplot(data=numerical_df, x=numerical_df.index, y=column, ax=axes[i], color='blue', label='Data')\n",
    "        if not outliers.empty:\n",
    "            sns.scatterplot(data=outliers, x=outliers.index, y=column, ax=axes[i], color='red', label='Outliers')\n",
    "        axes[i].set_title(f'Outliers in {column} (IQR Multiplier > {multiplier})')\n",
    "        \n",
    "    # Remove any unused axes\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "\n",
    "fig, axes = create_variable_grid(len(numerical_df.columns))\n",
    "plot_outliers_iqr(numerical_df, axes=axes)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35e89e",
   "metadata": {},
   "source": [
    "Outlier Plots:\n",
    "-   time_in_hospital:\n",
    "    -   Most values fall within a small range, with outliers concentrated at higher durations.\n",
    "    -   The distribution has visible horizontal bands due to discrete values.\n",
    "-   num_lab_procedures: \n",
    "    -  A high concentration of values in the lower range, with outliers extending well beyond 100.\n",
    "    -   Outliers are sparsely distributed, representing extreme values in lab procedures. \n",
    "-   num_procedures: \n",
    "    -   The data is heavily banded due to its discrete nature, with very few outliers visible.\n",
    "    -   Most values fall within the range of 0–5.\n",
    "-   num_medications: \n",
    "    -   A significant portion of the dataset exceeds the IQR threshold, indicating a substantial number of outliers.\n",
    "    -   Outliers dominate the upper range (values >30).\n",
    "-   number_outpatient: \n",
    "    -   Many outliers are visible, especially in the upper range, with values exceeding 40.\n",
    "    -   The majority of data points remain concentrated near zero.\n",
    "-   number_emergency: \n",
    "    -   The data is skewed toward zero, with a large number of extreme outliers above the IQR threshold.\n",
    "    -   Outliers are well-separated, forming a sparse tail.\n",
    "-   number_inpatient: \n",
    "    -   Similar to outpatient visits, most values are close to zero, with a growing number of outliers at higher inpatient counts.\n",
    "    -   Outliers appear spread out in the upper range.\n",
    "-   number_diagnoses: \n",
    "    -   Most values are clustered between 0 and 9, with a few significant outliers around 16.\n",
    "    -   Horizontal banding is due to discrete counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b4942",
   "metadata": {},
   "source": [
    "Summary of Outliers:\n",
    "Based on the QQ plots, most variables deviate from normality, with skewed distributions and heavy tails. Several variables have a considerable number of extreme values, especially `number_emergency`, `number_outpatient`, and `num_medications`. These outliers dominate the upper ranges and might bias our analysis if no action is taken. \n",
    "\n",
    "Outlier Handling:\n",
    "For highly skewed variables with significant outliers, we may need to consider capping/flooring or removing extreme outliers. Though we'll need to assess if this negatively impacts the honesty of our models. Additionally, we may be able to normalize/transform some of these values and extract some insgihts from them. For the discrete values, we may not be able to transform them, as they likely align with real-world observations. However, we may consider grouping rare values to simplify the data slightly. Finally, a potential solution could be engineering new features from the outliers (e.g., flagging hyper-critical emergency visits or highly medicated individuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff582e8-bea8-48e4-b401-d578c5d04172",
   "metadata": {},
   "source": [
    "#### 3.3.5 Data preprocessing: Explicitly set the order of ordinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f77049-f585-4158-b2dc-09b17003e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the correct order for each variable\n",
    "readmit_order = ['<30', '>30', 'NO']\n",
    "drug_order = ['No', 'Down', 'Steady', 'Up']\n",
    "max_glu_serum_order = ['Untested', 'Norm', '>200', '>300']\n",
    "a1cresult_order = ['Untested', 'Norm', '>7', '>8']\n",
    "age_order = ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)',\n",
    "             '[50-60)', '[60-70)', '[70-80)', '[80-90)', '[90-100)'] \n",
    "# # weight_order = ['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', \n",
    "# #                 '[125-150)', '[150-175)', '[175-200)', '>200'] # dropped this column\n",
    "\n",
    "# # List of drug-related variables\n",
    "drug_columns = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "                'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'tolazamide', \n",
    "                'pioglitazone', 'rosiglitazone', 'troglitazone', 'acarbose', 'miglitol', \n",
    "                'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "                'metformin-rosiglitazone', 'metformin-pioglitazone', 'glimepiride-pioglitazone']\n",
    "\n",
    "# # Reorder categories in the DataFrame\n",
    "df_clean['readmitted'] = pd.Categorical(df_clean['readmitted'], categories=readmit_order, ordered=True)\n",
    "df_clean['max_glu_serum'] = pd.Categorical(df_clean['max_glu_serum'], categories=max_glu_serum_order, ordered=True)\n",
    "df_clean['A1Cresult'] = pd.Categorical(df_clean['A1Cresult'], categories=a1cresult_order, ordered=True)\n",
    "df_clean['age'] = pd.Categorical(df_clean['age'], categories=age_order, ordered=True)\n",
    "# df_clean['weight'] = pd.Categorical(df_clean['weight'], categories=weight_order, ordered=True)\n",
    "\n",
    "for col in drug_columns:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.Categorical(df_clean[col], categories=drug_order, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f13a1-f097-4c0f-95d6-80641e396379",
   "metadata": {},
   "source": [
    "### 3.4 Visualization of the attributes\n",
    "\n",
    "#### 3.4.1 Target variable\n",
    "\n",
    "This plots the distribution of the target variable, readmitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceb1ae-cbf8-44e6-bbfc-f5658c6cb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette\n",
    "palette = sns.color_palette(\"muted\", n_colors=len(readmit_order))\n",
    "\n",
    "# Calculate counts and proportions of each category\n",
    "readmit_counts = df_clean['readmitted'].value_counts(sort=False)  # Counts\n",
    "readmit_proportions = readmit_counts / readmit_counts.sum()  # Proportions\n",
    "\n",
    "# Plot the distribution\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(\n",
    "    x=readmit_proportions.index, \n",
    "    y=readmit_proportions.values, \n",
    "    hue=readmit_proportions.index,\n",
    "    palette=palette, \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "ax.set_title('Distribution of Readmitted Categories')\n",
    "ax.set_xlabel('Readmitted')\n",
    "ax.set_ylabel('Proportion')\n",
    "\n",
    "# Add counts as labels on top of each bar\n",
    "for bar, count in zip(ax.patches, readmit_counts.values):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,  # x-coordinate\n",
    "        bar.get_height(),                  # y-coordinate (bar height)\n",
    "        f'{count}',                        # Text label\n",
    "        ha='center', va='bottom',         # Center alignment\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'plots/barplot_readmitted.png', bbox_inches=\"tight\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916adfac-f011-43b1-8868-590d7bb23a9a",
   "metadata": {},
   "source": [
    "#### 3.4.2 Numerical features\n",
    "\n",
    "This plots the distribution of each numerical variable split by response class. A histogram (or kernel density plot) and boxplots are plotted to get a visual sense of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9849d6-e909-44b0-ac12-57a3f460e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric columns\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Set the color palette\n",
    "palette = sns.color_palette(\"muted\", n_colors=len(readmit_order))\n",
    "\n",
    "# Generate plots for each numeric column\n",
    "for col in numeric_cols:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "    # Histogram with proportions\n",
    "    for idx, category in enumerate(readmit_order):\n",
    "        subset = df_clean[df_clean['readmitted'] == category]\n",
    "        sns.histplot(subset[col], bins=20, color=palette[idx], label=category, alpha=0.5, stat=\"probability\", ax=axes[0])\n",
    "\n",
    "    axes[0].set_title(f\"Histogram of {col} (Proportions) by Response Class\")\n",
    "    axes[0].set_xlabel(col)\n",
    "    axes[0].set_ylabel('Proportion')\n",
    "    axes[0].legend(title='Readmitted')\n",
    "\n",
    "    # OR Kernel Density Plot\n",
    "    # for idx, category in enumerate(readmit_order):\n",
    "    #     subset = df_clean[df_clean['readmitted'] == category]\n",
    "    #     sns.kdeplot(subset[col], fill=True, color=palette[idx], label=category, alpha=0.5, ax=axes[0])\n",
    "\n",
    "    # axes[0].set_title(f\"Kernel Density Plot of {col} by Response Class\")\n",
    "    # axes[0].set_xlabel(col)\n",
    "    # axes[0].set_ylabel('Density')\n",
    "    # axes[0].legend(title='Readmitted')\n",
    "\n",
    "    # Boxplot\n",
    "    sns.boxplot(data=df_clean, x='readmitted', y=col, hue='readmitted', palette=palette[:len(readmit_order)], ax=axes[1])\n",
    "    axes[1].set_title(f\"Boxplot of {col} by Response Class\")\n",
    "    axes[1].set_xlabel('Readmitted')\n",
    "    axes[1].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'plots/plot_{col}_by_readmitted.png', bbox_inches=\"tight\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75643b4d-8df8-452f-a41b-235aab2a12c8",
   "metadata": {},
   "source": [
    "Based on the skewness and large max-to-min ratio in several of the variables, it would be nice to see how the distributions look after log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1599033-ca08-4a88-9179-f8911a10d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the summary statistics, let's look at the log transformed distributions of a few variables.\n",
    "\n",
    "df_transform = df_clean.copy()\n",
    "\n",
    "# List of numeric columns\n",
    "transform_cols = ['num_lab_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient']\n",
    "\n",
    "# Generate plots for each numeric column\n",
    "for col in transform_cols:\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    # Histogram with log transformed proportions\n",
    "    for idx, category in enumerate(readmit_order):\n",
    "        subset = df_transform [df_transform ['readmitted'] == category]\n",
    "        sns.histplot(np.log1p(subset[col]), bins=20, color=palette[idx], label=category, alpha=0.5, stat=\"probability\", ax=axes[0])\n",
    "\n",
    "    axes[0].set_title(f'Histogram of Log {col} (Proportions) by Response Class')\n",
    "    axes[0].set_xlabel(f'log({col} + 1)')\n",
    "    axes[0].set_ylabel('Proportion')\n",
    "    axes[0].legend(title='Readmitted')\n",
    " \n",
    "    # Histogram with untransformed proportions\n",
    "    for idx, category in enumerate(readmit_order):\n",
    "        subset = df_transform[df_transform['readmitted'] == category]\n",
    "        sns.histplot(subset[col], bins=20, color=palette[idx], label=category, alpha=0.5, stat=\"probability\", ax=axes[1])\n",
    "\n",
    "    axes[1].set_title(f\"Histogram of {col} (Proportions) by Response Class\")\n",
    "    axes[1].set_xlabel(col)\n",
    "    axes[1].set_ylabel('Proportion')\n",
    "    axes[1].legend(title='Readmitted')\n",
    "\n",
    "    # OR Kernel Density Plot\n",
    "    # for idx, category in enumerate(readmit_order):\n",
    "    #     subset = df_transform [df_transform ['readmitted'] == category]\n",
    "    #     sns.kdeplot(np.log1p(subset[col]), fill=True, color=palette[idx], label=category, alpha=0.5, ax=axes[0])\n",
    "\n",
    "    # axes[0].set_title(f'Kernel Density Plot of log({col}) by Response Class')\n",
    "    # axes[0].set_xlabel(f'log({col} + 1)')\n",
    "    # axes[0].set_ylabel('Density')\n",
    "    # axes[0].legend(title='Readmitted')\n",
    "\n",
    "    # # Boxplot\n",
    "    # df_transform[f'log_{col}'] = np.log1p(df_transform[col])\n",
    "    # sns.boxplot(data=df_transform, x='readmitted', y=f'log_{col}', hue='readmitted', palette=palette[:len(readmit_order)], ax=axes[1])\n",
    "    # axes[1].set_title(f'Boxplot of log({col}) by Response Class')\n",
    "    # axes[1].set_xlabel('Readmitted')\n",
    "    # axes[1].set_ylabel(f'log({col} + 1)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'plots/plot_log({col})_by_readmitted.png', bbox_inches=\"tight\") \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f6dfd-f6bd-4f27-8a37-6804a73e7c4f",
   "metadata": {},
   "source": [
    "Log transformation doesn't seem to dramatically change or normalize the distributions, with the exception of perhap num_medications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d689dc-eedd-4f61-89bf-98750e16753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to get a sense of the importance of a variable in target class separation.\n",
    "\n",
    "# Group numerical variables by `readmitted` and view some summary stats\n",
    "\n",
    "# Split the numerical columns into two groups\n",
    "split_point = len(numeric_cols) // 2\n",
    "numeric_cols_part1 = numeric_cols[:split_point]\n",
    "numeric_cols_part2 = numeric_cols[split_point:]\n",
    "\n",
    "# Group numerical variables (first half) by `readmitted` and calculate summary stats\n",
    "aggregated_numstats1 = (\n",
    "    df_clean.groupby('readmitted', observed=False)[numeric_cols_part1]\n",
    "    .agg(['mean', 'std', 'median'])\n",
    "    .loc[['<30', '>30', 'NO']]  # Reorder rows for readability\n",
    ")\n",
    "\n",
    "# Group numerical variables (second half) by `readmitted` and calculate summary stats\n",
    "aggregated_numstats2 = (\n",
    "    df_clean.groupby('readmitted', observed=False)[numeric_cols_part2]\n",
    "    .agg(['mean', 'std', 'median'])\n",
    "    .loc[['<30', '>30', 'NO']]\n",
    ")\n",
    "\n",
    "print(\"Part 1: Summary statistics for the first group of numerical variables\")\n",
    "aggregated_numstats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf3ecb-ac0d-464a-a327-8dddd5975996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPart 2: Summary statistics for the second group of numerical variables\")\n",
    "aggregated_numstats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3820b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df['readmitted'] = df_clean['readmitted']\n",
    "\n",
    "# Aggregated Metrics Heatmap with Swapped Axes\n",
    "def plot_aggregated_metrics_heatmap(df, numerical_columns, response_column, metric='mean'):\n",
    "    # Aggregate the data by the response column\n",
    "    agg_df = df.groupby(response_column)[numerical_columns].agg(metric)\n",
    "\n",
    "    # Transpose the DataFrame to swap x and y axes\n",
    "    agg_df = agg_df.T\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        agg_df,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"coolwarm\",\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f'{metric.capitalize()} Values of Numeric Variables by {response_column}', fontsize=16)\n",
    "    plt.xlabel(response_column)  # Swapped\n",
    "    plt.ylabel('Numeric Variables')  # Swapped\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "plot_aggregated_metrics_heatmap(\n",
    "    df=numerical_df, \n",
    "    numerical_columns=numerical_df.columns[:-1], \n",
    "    response_column='readmitted', \n",
    "    metric='mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Metrics Heatmap with Swapped Axes\n",
    "def plot_aggregated_metrics_heatmap(df, numerical_columns, response_column, metric='std'):\n",
    "    # Aggregate the data by the response column\n",
    "    agg_df = df.groupby(response_column)[numerical_columns].agg(metric)\n",
    "\n",
    "    # Transpose the DataFrame to swap x and y axes\n",
    "    agg_df = agg_df.T\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        agg_df,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"coolwarm\",\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f'{metric.capitalize()} Values of Numeric Variables by {response_column}', fontsize=16)\n",
    "    plt.xlabel(response_column)  # Swapped\n",
    "    plt.ylabel('Numeric Variables')  # Swapped\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "plot_aggregated_metrics_heatmap(\n",
    "    df=numerical_df, \n",
    "    numerical_columns=numerical_df.columns[:-1], \n",
    "    response_column='readmitted', \n",
    "    metric='std'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated Metrics Heatmap with Swapped Axes\n",
    "def plot_aggregated_metrics_heatmap(df, numerical_columns, response_column, metric='median'):\n",
    "    # Aggregate the data by the response column\n",
    "    agg_df = df.groupby(response_column)[numerical_columns].agg(metric)\n",
    "\n",
    "    # Transpose the DataFrame to swap x and y axes\n",
    "    agg_df = agg_df.T\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        agg_df,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"coolwarm\",\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(f'{metric.capitalize()} Values of Numeric Variables by {response_column}', fontsize=16)\n",
    "    plt.xlabel(response_column)  # Swapped\n",
    "    plt.ylabel('Numeric Variables')  # Swapped\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "plot_aggregated_metrics_heatmap(\n",
    "    df=numerical_df, \n",
    "    numerical_columns=numerical_df.columns[:-1], \n",
    "    response_column='readmitted', \n",
    "    metric='median'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b5d73-b8a7-4154-b4f3-badc88c61334",
   "metadata": {},
   "source": [
    "- There isn't obvious class separation in the numerical variables. However, looking at the graphs and summary statistics, there does appear to be some difference in the following features.  \n",
    "  - time_in_hospital\n",
    "  - num_lab_procedures\n",
    "  - num_medications\n",
    "  - number_emergency\n",
    "  - number_inpatient\n",
    "  - number_diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521c2be",
   "metadata": {},
   "source": [
    "One more measure that we can use to assess the numeric values is an Empirical Cumulative Distribution Function. This graph represents the proportion of observations in the dataset that are less than or equal to a particular value. The x-axis shows the values of the variable and the y-axis shows the cumulative probability for that value. Each step represents an observation in the data, with the height of the step indicating the proporion of observations that have values less than or equal to the corresponding value on the x-axis. This will give us a visual summary of the data, including the extreme values, central tendencies, and spread to quickly understand what proportion of our data falls below certain thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'readmitted' in numerical_df:\n",
    "    del numerical_df['readmitted']\n",
    "\n",
    "def ecdf(data, axes):\n",
    "    \"\"\"\n",
    "    ECDF plot with percentile markers, confidence intervals, and summary metrics.\n",
    "    \"\"\"\n",
    "    # Sort data and compute ECDF\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(x) + 1) / len(x)\n",
    "\n",
    "    # Plot ECDF\n",
    "    axes.step(x, y, where=\"post\", color=\"blue\", label=\"ECDF\")\n",
    "    \n",
    "    # Highlight percentiles\n",
    "    percentiles = [0.25, 0.5, 0.75]\n",
    "    percentile_values = np.percentile(data, [25, 50, 75])\n",
    "    for p, value in zip(percentiles, percentile_values):\n",
    "        axes.axhline(p, linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
    "        axes.axvline(value, linestyle=\"--\", color=\"gray\", alpha=0.7)\n",
    "        axes.text(value, p, f\"{value:.2f}\", fontsize=9, verticalalignment=\"bottom\")\n",
    "\n",
    "    # Add confidence intervals\n",
    "    n = len(data)\n",
    "    ci_band = 1.36 / np.sqrt(n)  # Approximation for 95% CI\n",
    "    axes.fill_between(x, y - ci_band, y + ci_band, color=\"gray\", alpha=0.2, label=\"95% CI\")\n",
    "\n",
    "    # Annotate summary statistics\n",
    "    mean, median = np.mean(data), np.median(data)\n",
    "    axes.text(0.05, 0.9, f\"Mean: {mean:.2f}\\nMedian: {median:.2f}\",\n",
    "            transform=axes.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Add labels and title\n",
    "    axes.set_title(f\"ECDF: {column}\", fontsize=12)\n",
    "    axes.set_xlabel(\"Value\", fontsize=10)\n",
    "    axes.set_ylabel(\"Cumulative Probability\", fontsize=10)\n",
    "\n",
    "fig, axes = create_variable_grid(len(numerical_df.columns))\n",
    "for i, column in enumerate(numerical_df.columns):\n",
    "    ecdf(numerical_df[column].dropna(), axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba79201",
   "metadata": {},
   "source": [
    "Empirical Cumulative Distribution Function:\n",
    "-   time_in_hospital: \n",
    "    -   Mean: 4.40, Median: 4.00\n",
    "    -   Most patients stay for 2–6 days, with very few staying beyond 10 days.\n",
    "    -   The distribution is concentrated around the median, with a small tail extending toward longer durations.\n",
    "-   num_lab_procedures: \n",
    "    -   Mean: 43.10, Median: 44.00\n",
    "    -   The distribution is symmetric around 40–50 procedures, with most values between 20 and 80.\n",
    "    -   Few patients have lab procedures below 20 or above 100.\n",
    "-   num_procedures: \n",
    "    -   Mean: 1.34, Median: 1.00\n",
    "    -   Most patients have fewer than 2 procedures, with discrete steps indicating a small number of unique values.\n",
    "    -   Outliers are visible for values above 4 procedures.\n",
    "-   num_medications: \n",
    "    -   Mean: 16.02, Median: 15.00\n",
    "    -   The distribution is concentrated between 10 and 30 medications, with a gradual tail extending up to 80.\n",
    "    -   A large proportion of patients fall close to the mean.\n",
    "-   number_outpatient: \n",
    "    -   Mean: 0.37, Median: 0.00\n",
    "    -   The majority of patients (over 90%) have zero outpatient visits.\n",
    "    -   A small fraction of the population accounts for higher outpatient numbers, extending to a maximum of ~40.\n",
    "-   number_emergency: \n",
    "    -   Mean: 0.20, Median: 0.00\n",
    "    -   Similar to outpatient visits, most patients (over 95%) have zero emergency visits.\n",
    "    -   Outliers extend to values above 70, though they represent a very small percentage of the dataset.\n",
    "-   number_inpatient: \n",
    "    -   Mean: 0.64, Median: 0.00\n",
    "    -   Over 80% of patients have no inpatient visits, while the remaining show a gradual distribution up to ~20 visits.\n",
    "    -   A significant tail exists for higher values.\n",
    "-   number_diagnoses: \n",
    "    -   Mean: 7.42, Median: 8.00\n",
    "    -   The distribution is centered around the median, with most patients having between 6 and 9 diagnoses.\n",
    "    -   Very few patients have fewer than 4 or more than 12 diagnoses, making the distribution relatively compact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73ac08-b715-4c48-a404-36f07f2c6a7a",
   "metadata": {},
   "source": [
    "#### 3.4.3 Categorical features\n",
    "\n",
    "This plots the percentages of each readmitted class within the levels of each categorical variables in bar plots. Only the 12 most frequency categorical levels are displayed. It helps us get a sense of if and how the distribution of each response class changes between categorical levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe81db5-0e1b-422e-ba09-7c663fd7ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the color palette\n",
    "palette = sns.color_palette(\"muted\", n_colors=len(readmit_order))\n",
    "\n",
    "def calculate_proportions(df, catvar, response):\n",
    "    \"\"\"\n",
    "    Calculate proportions of response classes for each level of a categorical variable.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each combination of catvar and response\n",
    "    counts = df.groupby([catvar, response], observed=False).size().reset_index(name='count')\n",
    "    \n",
    "    # Calculate proportions within each level of the categorical variable\n",
    "    counts['proportion'] = counts.groupby(catvar, observed=False)['count'].transform(lambda x: x / x.sum())\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def plot_categorical_proportions(df, groups, response, ncols=2, top_n=12, show_empty=True):\n",
    "    \"\"\"\n",
    "    Create bar plots of proportions for categorical variables by response class.\n",
    "    \"\"\"\n",
    "    for group_name, variables in groups.items():\n",
    "        # Number of rows needed\n",
    "        nrows = -(-len(variables) // ncols)  # Ceiling division\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, nrows * 5))\n",
    "        axes = axes.flatten()  # Flatten axes for easy iteration\n",
    "        \n",
    "        # Plot each variable in the group\n",
    "        for i, var in enumerate(variables):\n",
    "            if var in df.columns:  # Check if variable exists in DataFrame\n",
    "                # Calculate proportions\n",
    "                proportions = calculate_proportions(df, var, response)\n",
    "                \n",
    "                # Optionally exclude empty categories\n",
    "                if not show_empty:\n",
    "                    # Remove rows with zero counts\n",
    "                    proportions = proportions[proportions['count'] > 0]\n",
    "\n",
    "                    # Dynamically adjust categories to remove unused ones\n",
    "                    if isinstance(proportions[var].dtype, pd.CategoricalDtype):\n",
    "                        proportions[var] = proportions[var].cat.remove_unused_categories()\n",
    "\n",
    "                # Limit to top_n categories by total count\n",
    "                top_categories = proportions.groupby(var, observed=False)['count'].sum().nlargest(top_n).index\n",
    "                plot_data = proportions[proportions[var].isin(top_categories)]\n",
    "                \n",
    "                # Bar plot\n",
    "                sns.barplot(\n",
    "                    data=plot_data,\n",
    "                    x=var,\n",
    "                    y='proportion',\n",
    "                    hue=response,\n",
    "                    palette=palette,\n",
    "                    ax=axes[i]\n",
    "                )\n",
    "                axes[i].set_title(f'{group_name}: {var}', fontsize=12)\n",
    "                axes[i].set_ylabel('Proportion')\n",
    "                axes[i].set_xlabel(var)\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "                axes[i].legend(title=response, loc='upper right')\n",
    "            else:\n",
    "                axes[i].set_visible(False)  # Hide empty plots\n",
    "        \n",
    "        # Hide unused axes\n",
    "        for j in range(len(variables), len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'Group: {group_name}', fontsize=16, y=1.02)  # Add a super title\n",
    "        # plt.savefig(f'plots/plot_{group_name}_by_readmitted.png', bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "# Grouping categorical variables for plotting\n",
    "groups = {\n",
    "    \"Demographics\": ['race', 'gender', 'age'], # excluded patient_nbr (and weight, which was dropped)\n",
    "    \"Administrative\": ['admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'payer_code', 'medical_specialty'],\n",
    "    \"Diagnoses\": ['diag_1', 'diag_2', 'diag_3'],\n",
    "    \"Tests\": ['max_glu_serum', 'A1Cresult'],\n",
    "    \"FirstLineDrugs\": ['insulin', 'metformin'],\n",
    "    \"Meglitinides\": ['repaglinide', 'nateglinide'],\n",
    "    \"Sulfonylureas\": ['chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'tolazamide'],\n",
    "    \"TZDs\": ['pioglitazone', 'rosiglitazone', 'troglitazone'],\n",
    "    \"AGIs\": ['acarbose', 'miglitol'],\n",
    "    \"Combos\": ['glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone'],\n",
    "    \"Treatment\": ['change', 'diabetesMed']\n",
    "}\n",
    "\n",
    "# Call the function to create bar plots\n",
    "plot_categorical_proportions(df_clean, groups, response='readmitted', ncols=2, top_n=12, show_empty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e923e68-b6af-44fb-9432-ea60521011d3",
   "metadata": {},
   "source": [
    "- Comparing the trends between the response classes across categorical levels, there are no stand-out examples of class separation between levels. We also need to bear in mind that many of these features have a large imbalance in their categorical levels (e.g. many of the drug variables). However, there are some differences where it looks like feature level may be associate with response class. Examples include:\n",
    "  - race\n",
    "  - admission_type_id\n",
    "  - discharge_disposition_id\n",
    "  - admission_source_id\n",
    "  - payer_code\n",
    "  - medical_specialty\n",
    "  - diag_1, diag_2, diag_3\n",
    "  - max_glu_serum\n",
    "  - nearly all the drug variables\n",
    "  - change\n",
    "  - diabetesMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cee67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df_clean.select_dtypes(include=['object', 'category'])\n",
    "subset_df = subset_df.drop(['medical_specialty', 'diag_1', 'diag_2', 'diag_3', 'patient_nbr', 'admission_type_id', 'max_glu_serum', 'A1Cresult'], axis=1)\n",
    "\n",
    "subset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'readmitted'  # Attribute of interest\n",
    "other_columns = [col for col in subset_df.columns if col != response]  # Exclude target\n",
    "\n",
    "crosstab_results = {}\n",
    "for column in other_columns:\n",
    "    crosstab_results[column] = pd.crosstab(subset_df[response], subset_df[column])\n",
    "\n",
    "def create_heatmaps(crosstab_results, response, normalization='none', n_cols=4, cmap=\"plasma\"):\n",
    "    \"\"\"\n",
    "    Creates a grid of heatmaps for given crosstab results.\n",
    "\n",
    "    Parameters:\n",
    "        crosstab_results (dict): Dictionary of crosstabs for each feature.\n",
    "        response (str): Name of the response variable.\n",
    "        normalization (str): Normalization type ('row', 'column', 'overall').\n",
    "        n_cols (int): Number of columns in the subplot grid.\n",
    "        cmap (str): Colormap for the heatmaps.\n",
    "    \"\"\"\n",
    "    n_features = len(crosstab_results)  # Total features to plot\n",
    "    n_rows = -(-n_features // n_cols)  # Compute rows by rounding up\n",
    "\n",
    "    # Create the figure and axes for subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows), sharex=False, sharey=False)\n",
    "    axes = np.ravel(axes)  # Flatten axes for consistent 1D handling\n",
    "\n",
    "    # Loop through crosstabs and create heatmaps\n",
    "    for i, (col, crosstab) in enumerate(crosstab_results.items()):\n",
    "        \n",
    "        # Normalize the crosstab based on the selected normalization\n",
    "        if normalization == 'none':\n",
    "            normalized_crosstab = crosstab \n",
    "        elif normalization == 'row':\n",
    "            normalized_crosstab = crosstab.div(crosstab.sum(axis=1), axis=0)    # Normalize by row\n",
    "        elif normalization == 'column':\n",
    "            normalized_crosstab = crosstab.div(crosstab.sum(axis=0), axis=1)    # Normalize by column\n",
    "        elif normalization == 'overall':\n",
    "            normalized_crosstab = crosstab / crosstab.values.sum()              # Normalize overall\n",
    "        else:\n",
    "            raise ValueError(\"Invalid normalization type. Choose 'none', row', 'column', or 'overall'.\")\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(normalized_crosstab, annot=True, fmt=\".2f\" if normalization != 'none' else \"d\", cmap=cmap, cbar=False, ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(f\"Crosstab of {response} vs {col}\")\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel(response)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# create_heatmaps(crosstab_results, response='readmitted', normalization='none', n_cols=4, cmap=\"plasma\")       #   Crosstab Heatmap: No Normalization, Raw Counts\n",
    "# create_heatmaps(crosstab_results, response='readmitted', normalization='row', n_cols=4, cmap=\"plasma\")        #   Crosstab Heatmap: No Normalization, Proportion of Factor Levels within the Response Variable\n",
    "# create_heatmaps(crosstab_results, response='readmitted', normalization='column', n_cols=4, cmap=\"plasma\")     #   Crosstab Heatmap: Column Normalization, Proportion of Factor Levels within the Predictor Variable\n",
    "create_heatmaps(crosstab_results, response='readmitted', normalization='overall', n_cols=3, cmap=\"plasma\")      #   Crosstab Heatmap: Overall Normalization, Proportion of Total Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36353f7d-7aaa-42be-bc25-b12dd694f880",
   "metadata": {},
   "source": [
    "#### 3.4.4 Feature selection\n",
    "\n",
    "Here, we want to do some feature selection to help get a sense of the most important features. First, we start by preprocessing categorical features, ordering the ordinal ones and then one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd29ab-36f0-44c8-a6d0-d77a5714e635",
   "metadata": {},
   "source": [
    "##### **A. Preprocessing for LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea5542-5171-4740-be16-7b0877331a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and dataset checks for LASSO feature selection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Create a copy of df_clean to avoid modifying the original dataframe\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Define the order for drug variables\n",
    "drug_order = {'No': 0, 'Down': 1, 'Steady': 2, 'Up': 3}\n",
    "\n",
    "# List of drug-related variables\n",
    "drug_columns = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "                'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'tolazamide', \n",
    "                'pioglitazone', 'rosiglitazone', 'troglitazone', 'acarbose', 'miglitol', \n",
    "                'insulin', 'glyburide-metformin', 'glipizide-metformin',\n",
    "                'metformin-rosiglitazone', 'metformin-pioglitazone', 'glimepiride-pioglitazone']\n",
    "\n",
    "# Apply the ordinal mapping to drug variables\n",
    "for col in drug_columns:\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col] = df_encoded[col].map(drug_order)\n",
    "\n",
    "# Encode the age ranges as ordinal\n",
    "df_encoded['age'] = pd.Categorical(df_encoded['age'], categories=age_order, ordered=True).codes\n",
    "\n",
    "# Preprocess diag_1, diag_2, diag_3 combining all \n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df_encoded[col] = df_encoded[col].str.split('.').str[0]  # Drop decimals and digits after\n",
    "# Group less frequent categories into other\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    top_categories = df_encoded[col].value_counts().nlargest(20).index # without this this code took forever to run\n",
    "    df_encoded[col] = df_encoded[col].apply(lambda x: x if x in top_categories else 'Other')\n",
    "\n",
    "# # Replace NaNs in 'medical_specialty' and 'payer_code' with 'unknown'\n",
    "# categorical_na_columns = ['medical_specialty', 'payer_code']\n",
    "# for col in categorical_na_columns:\n",
    "#     df_encoded[col] = df_encoded[col].fillna('unknown')\n",
    "\n",
    "# Drop highly missing columns\n",
    "columns_to_drop = ['patient_nbr', 'max_glu_serum', 'A1Cresult'] # weight is already dropped\n",
    "df_encoded = df_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "# Check for remaining missing values\n",
    "# print(\"Missing values before dropping rows:\")\n",
    "# print(df_encoded.isnull().sum())\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df_encoded = df_encoded.dropna()\n",
    "# Or impute them with the mode (or use KNN imputer)\n",
    "# for col in ['diag_1', 'diag_2', 'race']:\n",
    "#     df_encoded[col] = df_encoded[col].fillna(df_encoded[col].mode()[0])\n",
    "\n",
    "# Ensure the dataframe still has rows\n",
    "if df_encoded.shape[0] == 0:\n",
    "    raise ValueError(\"No rows left after dropping NaNs. Consider imputing missing values instead.\")\n",
    "\n",
    "# Check the shape of the final dataset\n",
    "# print(f\"Shape after preprocessing: {df_encoded.shape}\")\n",
    "\n",
    "# Define predictors (X) and response (y)\n",
    "X = df_encoded.drop(columns=['readmitted'])  # Exclude the target variable\n",
    "y = df_encoded['readmitted'].cat.codes  # Encode the target variable as 0, 1, 2\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),  # Scale numeric features\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)  # One-hot encode categorical features \n",
    "         # drop='first' avoids multicollinarity issues important for regression not RF\n",
    "         # handle_unknown='ignore' is important for sparse categories that might not show up in train and test splits\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to X\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Check the shape of the preprocessed dataset\n",
    "print(f\"Preprocessed dataset shape: {X_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d45e8-75e1-4614-97e2-c0bcd1f6d9a3",
   "metadata": {},
   "source": [
    "##### **B. LASSO penalized regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278066a7-86bc-41d2-b21f-f533fc20e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "# Fit LASSO logistic regression (LogisticRegressionCV with L1 penalty)\n",
    "lasso = LogisticRegressionCV(\n",
    "    penalty='l1',  # LASSO penalty\n",
    "    solver='saga',  # Supports L1 regularization\n",
    "    Cs=np.logspace(-3, 3, 20),  # Lambda values (inverse of regularization strength)\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='neg_log_loss',  # Metric: log loss\n",
    "    random_state=1234,\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1 #Use all CPU cores\n",
    ")\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best lambda (C is the inverse of lambda)\n",
    "best_lambda = 1 / lasso.C_[0]\n",
    "print(f\"Best lambda (penalty strength): {best_lambda}\")\n",
    "\n",
    "# Extract the feature names\n",
    "feature_names = (\n",
    "    list(numeric_features) +\n",
    "    list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n",
    ")\n",
    "\n",
    "# Extract the coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nLASSO Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "# Identify features not pushed to zero\n",
    "selected_features = coefficients[coefficients['Coefficient'] != 0]\n",
    "print(\"\\nSelected Features (Non-zero Coefficients):\")\n",
    "print(selected_features)\n",
    "\n",
    "# Evaluate the LASSO model\n",
    "y_pred = lasso.predict(X_test)\n",
    "y_pred_proba = lasso.predict_proba(X_test)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_pred_proba):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadf148-9fd2-4fd0-9839-875b82c39099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Separate numerical features before processing categorical ones\n",
    "numeric_features = coefficients['Feature'][coefficients['Feature'].isin(X.select_dtypes(include=['int64', 'float64']).columns)]\n",
    "categorical_features = coefficients['Feature'][~coefficients['Feature'].isin(numeric_features)]\n",
    "\n",
    "# Apply regex to remove one-hot encoding suffixes only for categorical features\n",
    "original_features = categorical_features.str.replace(r'_[^_]+$', '', regex=True)\n",
    "\n",
    "# Combine the cleaned categorical features with the numerical features\n",
    "unique_features = pd.concat([numeric_features, original_features]).unique()\n",
    "\n",
    "print(\"Unique Original Features:\")\n",
    "print(unique_features)\n",
    "\n",
    "# List of features in the dataset (from preprocessing)\n",
    "dataset_features = X.columns\n",
    "\n",
    "# Cleaned unique features (from LASSO coefficients)\n",
    "lasso_unique_features = pd.Series(unique_features)\n",
    "\n",
    "# Find common features\n",
    "common_features = lasso_unique_features[lasso_unique_features.isin(dataset_features)]\n",
    "\n",
    "# Find features in the dataset but not selected by LASSO\n",
    "unused_features = pd.Series(dataset_features[~dataset_features.isin(lasso_unique_features)])\n",
    "\n",
    "# # Print results\n",
    "# print(\"Common Features (Selected by LASSO and Present in Dataset):\")\n",
    "# print(common_features)\n",
    "\n",
    "print(\"\\nUnused Features (Present in Dataset but NOT Selected by LASSO):\")\n",
    "print(unused_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7860524-1f96-4b9b-936f-f2e4c6d63f4b",
   "metadata": {},
   "source": [
    "Penalized regression with LASSO only removes age.  \n",
    "\n",
    "Next we will try Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa53a1e-5c51-4708-a88b-a84eda94adf3",
   "metadata": {},
   "source": [
    "##### **C. Random forest for variable importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c01f76-ff31-4b9d-b8b3-c3f0fcc18dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define predictors (X) and response (y)\n",
    "X = df_encoded.drop(columns=['readmitted'])  # Exclude the target variable\n",
    "y = df_encoded['readmitted'].cat.codes  # Encode the target variable as 0, 1, 2\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),  # Scale numeric features\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing to X\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Check the shape of the preprocessed dataset\n",
    "print(f\"Preprocessed dataset shape: {X_preprocessed.shape}\")\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=1234)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Combine feature names and importances\n",
    "# Retrieve feature names from the preprocessor\n",
    "numeric_features = list(numeric_features)\n",
    "categorical_features = list(preprocessor.named_transformers_['cat'].get_feature_names_out())\n",
    "\n",
    "all_feature_names = numeric_features + categorical_features\n",
    "\n",
    "# Ensure the lengths match\n",
    "if len(all_feature_names) != len(importances):\n",
    "    raise ValueError(f\"Mismatch between feature names ({len(all_feature_names)}) and importances ({len(importances)}).\")\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# Set a threshold for importance (e.g., remove features with very low importance)\n",
    "threshold = 0.01  # Keep features with importance > 1%\n",
    "selected_features = feature_importances[feature_importances['Importance'] > threshold]\n",
    "\n",
    "print(\"\\nSelected Features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Filter X_train and X_test based on selected features\n",
    "selected_indices = selected_features.index\n",
    "X_train_selected = X_train[:, selected_indices]\n",
    "X_test_selected = X_test[:, selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce3b95-4ec3-47f2-b181-f6cc92c54d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Retrieve feature names from the preprocessor\n",
    "# For categorical features, OneHotEncoder creates feature names based on categories\n",
    "categorical_features_rf = preprocessor.named_transformers_['cat'].get_feature_names_out()\n",
    "numeric_features_rf = list(numeric_features)\n",
    "\n",
    "# Combine numeric and categorical feature names\n",
    "all_feature_names_rf = numeric_features_rf + list(categorical_features_rf)\n",
    "\n",
    "# Ensure the lengths match with the importance values\n",
    "if len(all_feature_names_rf) != len(importances):\n",
    "    raise ValueError(f\"Mismatch between feature names ({len(all_feature_names_rf)}) and importances ({len(importances)}).\")\n",
    "\n",
    "# Map selected feature indices back to their names\n",
    "selected_features_rf = feature_importances[feature_importances['Importance'] > threshold]\n",
    "selected_feature_indices = selected_features_rf.index\n",
    "selected_feature_names = [all_feature_names_rf[i] for i in selected_feature_indices]\n",
    "\n",
    "# Separate numerical and categorical selected features\n",
    "selected_numeric_features = [feature for feature in selected_feature_names if feature in numeric_features_rf]\n",
    "selected_categorical_features = [feature for feature in selected_feature_names if feature not in numeric_features_rf]\n",
    "\n",
    "# Apply regex to clean one-hot encoded names for categorical features only\n",
    "cleaned_selected_categorical_features = [re.sub(r'_[^_]+$', '', feature) for feature in selected_categorical_features]\n",
    "\n",
    "# Combine cleaned categorical features with numeric features\n",
    "unique_selected_features = pd.Series(selected_numeric_features + cleaned_selected_categorical_features).unique()\n",
    "\n",
    "print(\"Unique Original Features from Random Forest:\")\n",
    "print(unique_selected_features)\n",
    "\n",
    "# Compare with dataset features\n",
    "dataset_features_rf = list(X.columns)\n",
    "\n",
    "# Find common features\n",
    "common_rf_features = [feature for feature in unique_selected_features if feature in dataset_features_rf]\n",
    "\n",
    "# Find unused features in the dataset\n",
    "unused_rf_features = [feature for feature in dataset_features_rf if feature not in unique_selected_features]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nCommon Features (Selected by Random Forest and Present in Dataset):\")\n",
    "print(common_rf_features)\n",
    "\n",
    "print(\"\\nUnused Features (Present in Dataset but NOT Selected by Random Forest):\")\n",
    "print(unused_rf_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aba0b6-ec97-44fc-917d-2d94a5037915",
   "metadata": {},
   "source": [
    "After preprocessing the data, there are ~16 selected features. Age, the admission variables, and the medication variables are mostly excluded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377b666-57d7-4c81-bba5-ba5a2bd4391b",
   "metadata": {},
   "source": [
    "#### 3.4.5 Important features\n",
    "\n",
    "##### ***Visualize the most important attributes appropriately (at least 5 attributes). Important: Provide an interpretation for each chart. Explain for each attribute why the chosen visualization is appropriate.***\n",
    "\n",
    "*Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification).*\n",
    "\n",
    "This replots several of the variables that seemed important based on summary statistics, the plots above, and the random forest variable importance. To see the relationships a little differently, violin plots and stacked bar plots are used. Violin plots are appropriate visualizations for continuous variables. In addition to the median and IQR information also provide by boxplots, they reveal where data points are clustered thoroughout the distribution. The stacked bar plots are appropriate for categorical variables because they provide a side-by-side comparison of the proportion of each response class within each categorical level of the variable. This side-by-side visualization helps display where category may influence the response class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4201351-cf49-40ae-b5a0-e045e2e274c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "df_plotting = df_clean.copy()\n",
    "\n",
    "# Filter diag_1 and medical specialty to include only categories with more than 1000 occurrences\n",
    "diag_1_top_values = df_plotting['diag_1'].value_counts()\n",
    "diag_1_top_values = diag_1_top_values[diag_1_top_values > 1000].index\n",
    "df_plotting['diag_1_limited'] = df_plotting['diag_1'].where(df_plotting['diag_1'].isin(diag_1_top_values), other='Other')\n",
    "med_spec_top_values = df_plotting['medical_specialty'].value_counts()\n",
    "med_spec_top_values = med_spec_top_values[med_spec_top_values > 500].index\n",
    "df_plotting['medical_specialty_limited'] = df_plotting['medical_specialty'].where(df_plotting['medical_specialty'].isin(med_spec_top_values), other='Other')\n",
    "\n",
    "# Define numeric and categorical variables\n",
    "numVar = ['time_in_hospital', 'num_medications', 'number_emergency', 'number_inpatient']\n",
    "catVar = ['payer_code', 'diag_1_limited', 'medical_specialty_limited', 'discharge_disposition_id']\n",
    "\n",
    "# Violin Plots for Numeric Variables in Two Columns\n",
    "ncols = 2  # Number of columns in the grid\n",
    "nrows = -(-len(numVar) // ncols)  # Calculate the number of rows needed (ceiling division)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))  # Adjust size for clarity\n",
    "axes = axes.flatten()  # Flatten axes for easier indexing if nrows * ncols > len(numVar)\n",
    "\n",
    "for i, var in enumerate(numVar):\n",
    "    ax = axes[i]  # Get the appropriate subplot axis\n",
    "    sns.violinplot(\n",
    "        data=df_plotting, \n",
    "        x=var, \n",
    "        y='readmitted', \n",
    "        hue = 'readmitted',\n",
    "        order=readmit_order,\n",
    "        palette='muted', \n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f'Violin Plot of {var} by Readmitted')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Readmitted')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(numVar), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'plots/grouped_violinplots.png')\n",
    "plt.show()\n",
    "\n",
    "# Stacked Barplots for Categorical Variables in Two Columns\n",
    "ncols = 2  # Number of columns in the grid\n",
    "nrows = -(-len(catVar) // ncols)  # Calculate the number of rows needed (ceiling division)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n",
    "axes = axes.flatten()  # Flatten axes for easier indexing if nrows * ncols > len(catVar)\n",
    "\n",
    "for i, var in enumerate(catVar):\n",
    "    ax = axes[i]  # Get the appropriate subplot axis\n",
    "    # Calculate proportions of response classes within each level of the categorical variable\n",
    "    grouped_counts = (\n",
    "        df_plotting.groupby(var, observed=True)['readmitted']\n",
    "        .value_counts(normalize=True)\n",
    "        .unstack(fill_value=0) * 100\n",
    "    )\n",
    "    # Plot stacked barplot\n",
    "    grouped_counts[readmit_order].plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        ax=ax, \n",
    "        color=sns.color_palette(\"muted\"), \n",
    "        edgecolor='black',\n",
    "        width=0.8\n",
    "    )\n",
    "    ax.set_title(f'Stacked Barplot of {var} by Readmitted')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.legend(title='Readmitted', loc='upper right')\n",
    "    ax.tick_params(axis='x', rotation=0)  # Prevent rotation here for cleaner wrapping\n",
    "\n",
    "    # Dynamically fetch and wrap the x-tick labels\n",
    "    labels = [ '\\n'.join(wrap(label, 10)) for label in grouped_counts.index.astype(str)]\n",
    "    ax.set_xticks(range(len(labels)))  # Ensure tick positions match\n",
    "    ax.set_xticklabels(labels, rotation=90)  # Apply wrapped labels\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(len(catVar), len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'plots/stacked_barplots.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11f63a-79f3-4240-bcd9-504e7cb220e1",
   "metadata": {},
   "source": [
    "Observations on the Plots\n",
    "\n",
    "**Violin Plots:**\n",
    "\n",
    "- **time_in_hospital**: The distribution for early readmitted patients is smoother with a slightly higher median compared to those not readmitted. All distributions are right-skewed.\n",
    "- **num_medications**: There are subtle differences in the range and medians.\n",
    "  - Late readmissions have a slightly narrower range that the other two classes.\n",
    "  - The 'NO' class has the smallest median.\n",
    "  - Late readmitted has the largest median.\n",
    "- **number_emergency**: Differences appear in the tails of the distributions:\n",
    "  - The 'NO' class has the smallest range.\n",
    "  - Late readmitted has the largest range.\n",
    "  - Early readmitted falls in between. \n",
    "  This could suggest that late readmitted patients may experience more unexpected health challenges, often treated in emergency settings. Most observations fall between 0 and a few visits, but early readmitted patients have more observations in the \"few visits\" range compared to zero visits.\n",
    "- **number_inpatient**: The range increases across classes:\n",
    "  - The 'NO' class has the smallest range.\n",
    "  - Early readmitted has the largest range. \n",
    "  For the 'NO' class, most observations are at zero, whereas for early readmitted patients, the distribution becomes denser in the 0-5 range.\n",
    "\n",
    "**Stacked Bar Plots:**\n",
    "\n",
    "- **payer_code**: \n",
    "  - All observations from the 'FR' category are not readmitted, possibly indicating very few observations in this category.\n",
    "  - 'MP' and 'DM' show the smallest proportions of not readmitted cases.\n",
    "- **diag_1**: Among the 24 most frequent primary diagnosis codes plus an other category:\n",
    "  - Codes with the lowest proportions of not readmitted patients include 250.6, 428, 491, and 493.\n",
    "  - Code 715 has the highest proportion of not readmitted patients.\n",
    "  - Codes 786 and 435 have the fewest early readmitted patients.\n",
    "- **medical_specialty**: Among the 16 most frequent medical specialties plus other and unknown categories:\n",
    "  - Nephrology is the specialty with the lowest proportion of not readmitted and highest proportion of early readmitted patients.\n",
    "  - Specialties with the smallest proportion of early readmitted and largest proportion of not readmitted patients are Obstetrics/Gynecology and Surgery - Cardiovascular/Thoracic.\n",
    "- **discharge_disposition_id**: \n",
    "  - Categories 11, 19, and 20 show no readmitted observations, likely due to having very few total observations.\n",
    "  - Category 12 has the highest percentage of early readmitted patients and one of the lowest percentages of not readmitted patients, with no late readmitted observations.\n",
    "  - Category 15 shows the lowest proportion of not readmitted and among the highest of early readmitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff24ecce-5b20-4539-99d7-78e75c2bccd5",
   "metadata": {},
   "source": [
    "#### 3.4.6 Relationships between features\n",
    "\n",
    "##### ***Explore relationships between attributes: Look at the attributes via scatterplots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships.***\n",
    "\n",
    "**A. Scatterplot matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365de56-fb6c-4cc4-ac25-83dcc2d947cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix of the numeric variables\n",
    "\n",
    "# This doesn't work.\n",
    "# import warnings\n",
    "\n",
    "# # Suppress DeprecationWarning and FutureWarning\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "#     warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Sample 10% of the 100K record dataset\n",
    "sample_size = int(0.1 * len(df_clean))\n",
    "\n",
    "df_sampled = (\n",
    "    df_clean.groupby('readmitted', group_keys=False, observed=False)\n",
    "    .apply(lambda x: x.sample(frac=0.1))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Ensure the 'readmitted' column is present explicitly\n",
    "df_sampled = df_sampled.loc[:, df_clean.columns]\n",
    "\n",
    "# Add jitter to numeric variables\n",
    "df_sampled_jittered = df_sampled.copy()\n",
    "numeric_cols = df_sampled.select_dtypes(include=['int64']).columns\n",
    "\n",
    "df_sampled_jittered[numeric_cols] = df_sampled_jittered[numeric_cols].values + \\\n",
    "    np.random.rand(len(df_sampled_jittered), len(numeric_cols)) / 2\n",
    "\n",
    "# Create the pairplot\n",
    "sns.pairplot(\n",
    "    data=df_sampled_jittered,\n",
    "    hue=\"readmitted\",          # Group by 'readmitted'\n",
    "    diag_kws={\"common_norm\": False}, # Default ensures densities are group-specific    \n",
    ");\n",
    "# plt.savefig(f'plots/matrixScatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c632580-c402-4c39-850e-f61ff305c31d",
   "metadata": {},
   "source": [
    "Observations from the scatterplot:\n",
    "\n",
    "**Pairwise relationships**: There doesn't appear to be a strong linear trend between any pairs of variables, suggesting that the numeric variables aren't strongly correlated and multicollearity isn't much of a concern.  \n",
    "\n",
    "**Distributions**: The kernel density plots show that many of the variables (e.g. time_in_hospital, number_outpatient, and number_emergency) appear to have skewed distributions, with a majority of values concentrated at the lower end of the range. num_lab_procedures has a more symmetric distribution compared to other variables, with a relatively broader range of values.\n",
    "\n",
    "**Outliers**: Some variables (e.g. number_outpatient, number_emergency, and number_diagnoses) have extreme outlier values. These should be explored further.\n",
    "\n",
    "**Relationship with the target variable**: The hospital visit variables (e.g. number_outpatient, number_emergency and number_inpatient) have more data points in the readmitted '<30' and '>30' categories with higher values, which could indicate that frequent hospital visits correlate with the likelihood of readmission. num_medications appear less strongly related to readmitted categories compared to visit-related metrics, given the spread of the not readmitted data points throughout the range.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013ed86-f5d5-4d50-8c73-4601db0fe444",
   "metadata": {},
   "source": [
    "**B. Correlation matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78989d5-0cc1-489b-b01b-b62c1a07c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for the correlation matrix\n",
    "numeric_df = df_clean.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Create the plot\n",
    "f, ax = plt.subplots(figsize=(8, 8))  # Figure and axes\n",
    "sns.heatmap(corr_matrix,\n",
    "            annot=True,         # Plot numeric annotations\n",
    "            fmt=\".2f\",          # Format for annotations\n",
    "            square=True,        # Keep cells square\n",
    "            cmap=sns.color_palette(\"coolwarm\", as_cmap=True),\n",
    "            cbar_kws={\"shrink\": 0.7},  # Colorbar customization\n",
    "            ax=ax)              # Use the same axis\n",
    "\n",
    "# Adjust layout\n",
    "f.tight_layout()\n",
    "# plt.savefig(f'plots/correlationplt.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902a2aa-24a8-4317-9a58-b59911d97219",
   "metadata": {},
   "source": [
    "From the correlation plot, the numerical variables don't show a lot of evidence of correlation or multicollinearity. `num_medications` is somewhat correlated with `time_in_hospital` (0.47) and `num_procedures` (0.39)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c24ff0-fc45-4ed4-9965-a2092925bd5d",
   "metadata": {},
   "source": [
    "**C. Cross tabulations**\n",
    "\n",
    "It would be interesting to get a sense of the discharges by payer. Are some providers more likely to pay for rehabilitation or other services after discharge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70f328-1eb1-4df9-9a37-034803b31059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only categories with more than 2000 occurrences\n",
    "discharge_top_values = df_plotting['discharge_disposition_id'].value_counts()\n",
    "discharge_top_values = discharge_top_values[discharge_top_values > 2000].index\n",
    "\n",
    "# Ensure 'Other' is included in the categories\n",
    "if 'Other' not in df_plotting['discharge_disposition_id'].cat.categories:\n",
    "    df_plotting['discharge_disposition_id'] = df_plotting['discharge_disposition_id'].cat.add_categories(['Other'])\n",
    "\n",
    "# Apply the where condition to limit categories\n",
    "df_plotting['discharge_id_limited'] = df_plotting['discharge_disposition_id'].where(\n",
    "    df_plotting['discharge_disposition_id'].isin(discharge_top_values), \n",
    "    other='Other'\n",
    ")\n",
    "\n",
    "# Create cross-tab\n",
    "discharge_crosstab = pd.crosstab(df_plotting['payer_code'], df_plotting['discharge_id_limited'])\n",
    "print(discharge_crosstab)\n",
    "\n",
    "# Normalize the cross-tab to calculate rates\n",
    "discharge_rate = discharge_crosstab.div(discharge_crosstab.sum(1).astype(float), axis=0)\n",
    "\n",
    "# Plot the cross-tab and normalized rates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot cross-tab\n",
    "discharge_crosstab.plot(kind='bar', stacked=True, colormap='Spectral_r', ax=axes[0])\n",
    "axes[0].set_title('Cross-tab: Payer Code vs. Discharge ID')\n",
    "axes[0].set_ylabel('Counts')\n",
    "axes[0].set_xlabel('Payer Code')\n",
    "\n",
    "# Plot normalized rates\n",
    "discharge_rate.plot(kind='bar', stacked=True, colormap='Spectral_r', ax=axes[1])\n",
    "axes[1].set_title('Normalized Rates: Payer Code vs. Discharge ID')\n",
    "axes[1].set_ylabel('Proportions')\n",
    "axes[1].set_xlabel('Payer Code')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'plots/crosstab_payer_discharge.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ac251-4b54-4cba-9a5a-28a4ead29650",
   "metadata": {},
   "source": [
    "This displays the most frequent discharge codes for each payer. The discharge code descriptions follow:\n",
    "- 1: Discharged to home\n",
    "- 2: Discharged/transferred to another short-term hospital\n",
    "- 3: Discharged/transferred to SNF\n",
    "- 6: Discharged/transferred to home with home health service\n",
    "- 18: NULL (I think this is different than 'died'. The documentation has other categories for 'expired'.)\n",
    "\n",
    "'MC' patients have the smallest proportion discharged to home and with `CM` the largest discharged to SNF, which may be a supervised nursing facility. There is quite a bit of variability between these two variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89992e2b-45a6-4b59-ba55-d9f59c1686a0",
   "metadata": {},
   "source": [
    "Additionally, are some payers more likely to have patients that get emergency treatment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181abec-6a13-4630-9dfb-d053ba9ecfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges and labels\n",
    "bins = [0, 1, 3, 5, 10, float('inf')]  # Define bin edges\n",
    "labels = ['0', '1-3', '4-5', '6-10', '11+']  # Define category labels\n",
    "\n",
    "# Bin the 'number_emergency' variable\n",
    "df_clean['number_emergency_binned'] = pd.cut(df_clean['number_emergency'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Create cross-tab\n",
    "emergency_crosstab = pd.crosstab(df_clean['payer_code'], df_clean['number_emergency_binned'])\n",
    "print(emergency_crosstab)\n",
    "\n",
    "# Normalize the cross-tab to calculate rates\n",
    "emergency_rate = emergency_crosstab.div(emergency_crosstab.sum(1).astype(float), axis=0)\n",
    "\n",
    "# Plot the cross-tab and normalized rates\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot cross-tab\n",
    "emergency_crosstab.plot(kind='bar', stacked=True, colormap='Spectral_r', ax=axes[0])\n",
    "axes[0].set_title('Cross-tab: Payer Code vs. Number of Emergencies')\n",
    "axes[0].set_ylabel('Counts')\n",
    "axes[0].set_xlabel('Payer Code')\n",
    "\n",
    "# Plot normalized rates\n",
    "emergency_rate.plot(kind='bar', stacked=True, colormap='Spectral_r', ax=axes[1])\n",
    "axes[1].set_title('Normalized Rates: Payer Code vs. Number of Emergencies')\n",
    "axes[1].set_ylabel('Proportions')\n",
    "axes[1].set_xlabel('Payer Code')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'plots/crosstab_payer_emergencies.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b3900-6d56-4dc4-9a0e-0b528c37ca12",
   "metadata": {},
   "source": [
    "From earlier, we saw that the `DM` payer had one of the smallest proportions of the not readmitted class. Interestingly from the cross tab here, DM also appears to be associated with the most number of emergency visits and the smallest proportion of patients with zero emergency visits.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514c6a1-300d-444d-b501-461afbd44716",
   "metadata": {},
   "source": [
    "##### ***Are there other features that could be added to the data or created from existing features? Which ones?***\n",
    "\n",
    "Some options for combining features for simplicity are to group drugs by classes and group diagnosis codes by health category (e.g. cardiovascular, endocrine, etc.). Variables with many categorical levels could be reorganized to group sparce levels into an other category (e.g. payer_code, medical_specialty, and discharge_disposition_id). Another option to consider would be restricting the analysis to adults over 20 years old. The dataset primarily consists of adults and it seems logical that the disease, treatments and outcomes of children might be different than adults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5890fb-804a-48ff-878d-a6615b4ad1c8",
   "metadata": {},
   "source": [
    "#### 3.4.5 One hot encoding and dimensionality reduction\n",
    "\n",
    "##### ***Additional analyses (e.g. implement dimensionality reduction, then visualize and interpret the results).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b2670",
   "metadata": {},
   "source": [
    "**A. One hot encoding and PCA: First Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.read_csv(\"C:/Users/rhine/Desktop/Southern Methodist University - Coursework/3 - Spring 2025/DS-7331 Machine Learning 1/1/diabetes+130-us+hospitals+for+years+1999-2008/diabetic_data.csv\")\n",
    "pca_df = pca_df.drop(['encounter_id', 'patient_nbr','weight','max_glu_serum','A1Cresult'], axis=1)\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c132a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that may not contribute to analysis\n",
    "high_cardinality_columns = [col for col in pca_df.columns if pca_df[col].nunique() > 9]\n",
    "\n",
    "# Step 1: Create frequency maps for all columns\n",
    "frequency_maps = {col: pca_df[col].value_counts() for col in high_cardinality_columns}\n",
    "\n",
    "# Step 2: Apply frequency encoding\n",
    "for col in high_cardinality_columns:\n",
    "    pca_df[f'{col}_encoded'] = pca_df[col].map(frequency_maps[col])\n",
    "\n",
    "# Apply encoding with fallback for unseen categories\n",
    "for col in high_cardinality_columns:\n",
    "    pca_df[f'{col}_encoded'] = pca_df[col].map(frequency_maps[col]).fillna(0)\n",
    "\n",
    "print(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping old high cardinality columns\n",
    "pca_df.drop(high_cardinality_columns, axis=1, inplace=True)\n",
    "\n",
    "# Identify all new columns that have 'encoded' in their names\n",
    "high_cardinality_columns = [col for col in pca_df.columns if 'encoded' in col or 'readmitted' in col]\n",
    "\n",
    "\n",
    "categorical_columns = pca_df.select_dtypes(include='object').columns\n",
    "categorical_columns = categorical_columns[0:26]\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7624d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding to all categorical columns\n",
    "df_encoded = pd.get_dummies(pca_df.drop(columns=high_cardinality_columns + ['readmitted']), \n",
    "                            columns=categorical_columns, \n",
    "                            drop_first=False)\n",
    "\n",
    "df_encoded = pd.concat([pca_df[high_cardinality_columns] , pca_df[['readmitted']], df_encoded], axis=1)\n",
    "\n",
    "# Updated dataset\n",
    "print(df_encoded.info()) \n",
    "print(df_encoded.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa74d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove duplicate columns\n",
    "df_encoded = df_encoded.loc[:, ~df_encoded.T.duplicated()]\n",
    "\n",
    "# Check for columns with missing values\n",
    "missing_summary = df_encoded.isnull().sum()\n",
    "print(missing_summary[missing_summary > 0])  # Columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize all categorical variables\n",
    "\n",
    "# Select all boolean columns\n",
    "categorical_columns = list(df_encoded.select_dtypes(include='bool').columns)\n",
    "\n",
    "# Append 'readmitted' column to the list (if it exists in the DataFrame)\n",
    "if 'readmitted' in df_encoded.columns:\n",
    "    categorical_columns.append('readmitted')\n",
    "\n",
    "# Factorize    \n",
    "for col in categorical_columns:\n",
    "    df_encoded[col], mapping = pd.factorize(df_encoded[col])\n",
    "\n",
    "print(df_encoded.info())  \n",
    "print(df_encoded.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_and_dtypes_df = df_encoded.dtypes.reset_index()\n",
    "columns_and_dtypes_df.columns = ['Column', 'Data Type']\n",
    "print(columns_and_dtypes_df)\n",
    "\n",
    "Left_out = pca_df[['readmitted','diabetesMed']]\n",
    "\n",
    "# Factorize    \n",
    "for col in Left_out:\n",
    "    df_encoded[col], mapping = pd.factorize(df_encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Quick feature importance analysis\n",
    "X = df_encoded.drop(['readmitted'], axis=1)\n",
    "y = df_encoded['readmitted']\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feature_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "for feature,importance in feature_importance.items():\n",
    "     if importance >= .02:\n",
    "         print(f\"{feature}: {importance}\")\n",
    "         \n",
    "# Save feature names with importance >= 0.02 into a list\n",
    "important_features = feature_importance[feature_importance >= 0.02].index.tolist()\n",
    "\n",
    "# Now, `important_features` contains the feature names\n",
    "print(important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15150d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# PCA for Feature Reduction ##############################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Preprocessing - Standardize the Data\n",
    "# Assuming X is your feature matrix\n",
    "df_reduced = pca_df[important_features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_reduced)  # Standardize the features\n",
    "\n",
    "# Step 2: Fit PCA\n",
    "pca = PCA(n_components=13)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Inspect Results\n",
    "explained_variance_ratio = pca.explained_variance_ratio_  # Variance explained by each component\n",
    "n_components = pca.n_components_  # Number of components chosen\n",
    "\n",
    "print(f\"Number of components selected: {n_components}\")\n",
    "print(f\"Explained variance ratio: {explained_variance_ratio}\")\n",
    "print(f\"Total explained variance: {sum(explained_variance_ratio):.2f}\")\n",
    "\n",
    "# Step 4: Create a DataFrame for PCA-transformed data\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "# Save for further analysis\n",
    "X_pca_df.to_csv(\"pca_transformed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, marker='o', linestyle='--')\n",
    "plt.title(\"Explained Variance by Principal Components\")\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.1)\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.colorbar(label=\"Readmitted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be044cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plot PC1 vs PC2 by response class\n",
    "\n",
    "pc1 = X_pca[:, 0]  # First principal component\n",
    "pc2 = X_pca[:, 1]  # Second principal component\n",
    "\n",
    "Read_Reduced = df_encoded['readmitted']\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': pc1,\n",
    "    'PC2': pc2,\n",
    "    'Readmitted': Read_Reduced\n",
    "})\n",
    "\n",
    "# Perform stratified sampling (10%)\n",
    "pca_sample, _ = train_test_split(\n",
    "    pca_df,\n",
    "    test_size=0.9,  # Keep 10% of the data\n",
    "    stratify=pca_df['Readmitted'],  # Stratify by the 'Readmitted' column\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot with the stratified sample\n",
    "sns.scatterplot(\n",
    "    data=pca_sample,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Readmitted',\n",
    "    palette='muted',\n",
    "    alpha=0.7,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Scatter Plot (10% Stratified Sample)')\n",
    "ax1.set_xlabel('Principal Component 1 (PC1)')\n",
    "ax1.set_ylabel('Principal Component 2 (PC2)')\n",
    "ax1.legend(title='Readmitted')\n",
    "\n",
    "# Get axis limits from scatterplot\n",
    "x_limits = ax1.get_xlim()\n",
    "y_limits = ax1.get_ylim()\n",
    "\n",
    "# KDE plot with full data, matching axis limits\n",
    "sns.kdeplot(\n",
    "    data=pca_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Readmitted',\n",
    "    palette='muted',\n",
    "    ax=ax2,\n",
    "    legend=True\n",
    ")\n",
    "ax2.set_title('KDE Plot (Matched Axis Limits)')\n",
    "ax2.set_xlabel('Principal Component 1 (PC1)')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# Apply matching axis limits\n",
    "ax2.set_xlim(x_limits)\n",
    "ax2.set_ylim(y_limits)\n",
    "\n",
    "# Add a figure-wide title\n",
    "fig.suptitle('PC1 vs. PC2 by Readmitted', fontsize=14)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust rect to prevent overlap with suptitle\n",
    "# plt.savefig('plots/PC1vPC2_by_readmitted.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a524f12-f8d3-48a2-9fdd-c089b0f9e38f",
   "metadata": {},
   "source": [
    "**B. One-Hot Encoding and PCA: Alternative Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235668de-446e-42b9-8a00-ef1465edfcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with many missing values and patient IDs\n",
    "columns_to_drop = ['patient_nbr'] #, 'weight', 'max_glu_serum', 'A1Cresult']\n",
    "df_encoded = df_clean.drop(columns= columns_to_drop)\n",
    "\n",
    "# Preprocess diag_1, diag_2, diag_3 combining all \n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df_encoded[col] = df_encoded[col].str.split('.').str[0]  # Drop decimals and digits after\n",
    "# print(df_encoded[['diag_1', 'diag_2', 'diag_3']].head(20))\n",
    "\n",
    "# # Replace NaNs in 'medical_specialty' and 'payer_code' with 'unknown'\n",
    "# categorical_na_columns = ['medical_specialty', 'payer_code']\n",
    "# for col in categorical_na_columns:\n",
    "#     df_encoded[col] = df_encoded[col].fillna('unknown')\n",
    "\n",
    "# Drop rows with NaN values (race and diagnoses cols)\n",
    "df_encoded = df_encoded.dropna()\n",
    "\n",
    "# Remove target (split into X and y, for plotting with y later)\n",
    "X_df_encoded = df_encoded.drop(columns='readmitted')\n",
    "y_df_encoded = df_encoded['readmitted']\n",
    "\n",
    "# Handle categorical variables with one-hot encoding\n",
    "categorical_columns = X_df_encoded.select_dtypes(include=['object', 'category']).columns\n",
    "X_df_encoded = pd.get_dummies(X_df_encoded, columns=categorical_columns, drop_first=False)\n",
    "\n",
    "# Standardize the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb780ab-3a26-49f2-8c93-61feaa8b7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()  # Keep all components initially\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Plot cumulative variance\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid()\n",
    "# plt.savefig(f'plots/PCAexplainedVariance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ead63c-2828-4c44-b5bb-4df2ce6e2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of components to explain ~80% of variance\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1  # +1 because indices start at 0\n",
    "print(f\"Number of original features: {X_df_encoded.shape[1]}\\nNumber of components to explain ~80% of variance: {n_components_80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e04e9d-123a-4690-9690-42c64a73568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plot PC1 vs PC2 by response class\n",
    "\n",
    "pc1 = X_pca[:, 0]  # First principal component\n",
    "pc2 = X_pca[:, 1]  # Second principal component\n",
    "\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': pc1,\n",
    "    'PC2': pc2,\n",
    "    'Readmitted': y_df_encoded\n",
    "})\n",
    "\n",
    "# Perform stratified sampling (10%)\n",
    "pca_sample, _ = train_test_split(\n",
    "    pca_df,\n",
    "    test_size=0.9,  # Keep 10% of the data\n",
    "    stratify=pca_df['Readmitted'],  # Stratify by the 'Readmitted' column\n",
    "    random_state=1234\n",
    ")\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plot with the stratified sample\n",
    "sns.scatterplot(\n",
    "    data=pca_sample,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Readmitted',\n",
    "    palette='muted',\n",
    "    alpha=0.7,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('Scatter Plot (10% Stratified Sample)')\n",
    "ax1.set_xlabel('Principal Component 1 (PC1)')\n",
    "ax1.set_ylabel('Principal Component 2 (PC2)')\n",
    "ax1.legend(title='Readmitted')\n",
    "\n",
    "# Get axis limits from scatterplot\n",
    "x_limits = ax1.get_xlim()\n",
    "y_limits = ax1.get_ylim()\n",
    "\n",
    "# KDE plot with full data, matching axis limits\n",
    "sns.kdeplot(\n",
    "    data=pca_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    hue='Readmitted',\n",
    "    palette='muted',\n",
    "    ax=ax2,\n",
    "    legend=True\n",
    ")\n",
    "ax2.set_title('KDE Plot')\n",
    "ax2.set_xlabel('Principal Component 1 (PC1)')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# Apply matching axis limits\n",
    "ax2.set_xlim(x_limits)\n",
    "ax2.set_ylim(y_limits)\n",
    "\n",
    "# Add a figure-wide title\n",
    "fig.suptitle('PC1 vs. PC2 by Readmitted', fontsize=14)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust rect to prevent overlap with suptitle\n",
    "# plt.savefig('plots/PC1vPC2_by_readmitted.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9546bc-6e7b-4060-8bfd-ca0a2fe4e89b",
   "metadata": {},
   "source": [
    "These plots display only the first two principal components (PCs) out of the 1690 required to explain 80% of the variance. While these two PCs do not separate the response classes, they reveal distinct patterns in the data distribution:\n",
    "- Non-readmitted patients spread widely extending to the exterior boundaries of the plot.  \n",
    "- Early readmitted patients cluster more densely near the center, suggesting a smaller variance in their principal component scores.  \n",
    "- Late readmitted patients have a broader distribution than early readmitted patients but less spread than the non-readmitted group.  \n",
    "\n",
    "Although the PCs are less interpretable, PCA successfully reduced the dataset from the original 2453 one-hot-encoded features. This dimensionality reduction may be useful for further analyses and modeling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
